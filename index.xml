<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blake Wulfe</title>
    <link>https://wulfebw.github.io/</link>
      <atom:link href="https://wulfebw.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Blake Wulfe</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 22 Dec 2020 16:25:15 -0800</lastBuildDate>
    <image>
      <url>https://wulfebw.github.io/images/icon_hu74c5e83fe8339e265cecdcef41753095_57754_512x512_fill_lanczos_center_2.png</url>
      <title>Blake Wulfe</title>
      <link>https://wulfebw.github.io/</link>
    </image>
    
    <item>
      <title>Everything I tried and learned in the NeurIPS 2020 Procgen Competition</title>
      <link>https://wulfebw.github.io/post/procgen/</link>
      <pubDate>Tue, 22 Dec 2020 16:25:15 -0800</pubDate>
      <guid>https://wulfebw.github.io/post/procgen/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I participated in the 
&lt;a href=&#34;https://www.aicrowd.com/challenges/neurips-2020-procgen-competition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS 2020 Procgen Challenge&lt;/a&gt; with my colleague, Adrien Gaidon, and ended up winning the competition (1st place in the sample efficiency track, 2nd place in the generalization track).&lt;/li&gt;
&lt;li&gt;This post describes the process I followed, the methods I tried, and lessons learned from the competition.&lt;/li&gt;
&lt;li&gt;Here&amp;rsquo;s a brief video giving an overview:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/RwEqFuc6Qt0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The competition evaluates reinforcement learning agents on the 
&lt;a href=&#34;https://github.com/openai/procgen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Procgen environments&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;There are two tracks:
&lt;ul&gt;
&lt;li&gt;Sample efficiency: For each env, train and evaluate on the full distribution of levels.&lt;/li&gt;
&lt;li&gt;Generalization: For each env, train on 200 levels, and evaluate on the full distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Participants submit code to train agents. Agents are trained for each environment, and the scores across environments are aggregated to get the final score.
&lt;ul&gt;
&lt;li&gt;The final evaluation is on 16 public environments (this counted for half the score), and 4 private environments (the other half of the weight in the score).&lt;/li&gt;
&lt;li&gt;The agents only have 8 million steps in the environments during training, and can only run training for at most 2 hours&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;process&#34;&gt;Process&lt;/h2&gt;
&lt;h3 id=&#34;for-deciding-which-method-to-try-next&#34;&gt;For deciding which method to try next&lt;/h3&gt;
&lt;p&gt;This is basically &amp;ldquo;use the scientific method&amp;rdquo;, but there are a couple aspects specific to RL:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evaluate the performance of the current best approach
&lt;ul&gt;
&lt;li&gt;Plotting learning curves on train / evaluation levels&lt;/li&gt;
&lt;li&gt;Visualizing the policy at different points during training
&lt;ul&gt;
&lt;li&gt;Watch the policy and try to understand why it doesn&amp;rsquo;t perform better / where it gets stuck / killed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;List all the reasons the agent might be performing suboptimally in a given environment or across environments
&lt;ul&gt;
&lt;li&gt;See Appendix 1 for an outline of reasons&lt;/li&gt;
&lt;li&gt;This often involves doing a literature review / looking for relevant papers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Choose a reason, and design an experiment to check if that really is the reason
&lt;ul&gt;
&lt;li&gt;Often the experiment is trying out some method to address the problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repeat&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;for-checking-whether-an-approach-is-implemented-correctly&#34;&gt;For checking whether an approach is implemented correctly&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This is the process I followed after implementing an approach / method to check if I implemented it correctly
&lt;ul&gt;
&lt;li&gt;In RL it&amp;rsquo;s often difficult to tell if you correctly implemented something because often the agent will perform well / behave reasonably even if the method is implemented incorrectly
&lt;ul&gt;
&lt;li&gt;The extent to which this is the case varies based on what you&amp;rsquo;re implementing (e.g., getting the training algorithm wrong will generally be more noticeable than getting an exploration strategy wrong)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Run training on a single, easy level in Coinrun&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I found an easy training level by sequentially going through the levels on Coinrun:
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m procgen.interactive --env-name coinrun --level-seed 4 --distribution-mode easy
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;The level just involves learning to move to the right:
&lt;figure class=img-fig&gt;
   &lt;img src=&#34;easy_coinrun.png&#34; alt=&#34;grayscale&#34; class=&#34;img-sm&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;Run training for ~5 minutes and if the agent hasn&amp;rsquo;t learned to move right, then assume there&amp;rsquo;s a bug
&lt;ul&gt;
&lt;li&gt;This works when using PPO, but it doesn&amp;rsquo;t work when using some other methods
&lt;ul&gt;
&lt;li&gt;DQN is particularly slow to learn early in training, so it can take much longer to pass this sanity check when using it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run training on multiple levels of Coinrun&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I used Coinrun because it&amp;rsquo;s probably the easiest environment&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run training on all (or relevant subset) of envs&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;for-running-experiments&#34;&gt;For running experiments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The competition is based on the 
&lt;a href=&#34;https://docs.ray.io/en/master/rllib.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rllib package&lt;/a&gt;, which is built on top of 
&lt;a href=&#34;https://docs.ray.io/en/master/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ray&lt;/a&gt;, so I used ray for running hyperparam searches / experiments.
&lt;ul&gt;
&lt;li&gt;ray tune has a 
&lt;a href=&#34;https://simon-ray.readthedocs.io/en/doc-new-theme/tune/api_docs/execution.html#tune-run-experiments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;run_experiments&lt;/code&gt;&lt;/a&gt; function, that runs a set of experiments, where those experiments can be specified as a set of configuration files&lt;/li&gt;
&lt;li&gt;So I basically wrote a script to generate the configuration files for experiments I want to run
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/wulfebw/neurips2020-procgen/blob/master/multi_train.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&amp;rsquo;s&lt;/a&gt; the script
&lt;ul&gt;
&lt;li&gt;The script uses a 
&lt;a href=&#34;https://stackoverflow.com/questions/9098194/name-parts-of-iterables-in-itertools-products&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;(named) cross product&lt;/a&gt; for grid searches:
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from itertools import product, starmap
from collections import namedtuple

def named_product(**items):
    Product = namedtuple(&#39;Product&#39;, items.keys())
    return starmap(Product, product(*items.values()))
            
...
def sample_configs(...):
    parameter_settings = named_product(
        learning_rate=[1e-4, 1e-3],
        num_sgd_iter=[1, 2],
        ...,
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;This gives a &lt;code&gt;namedtuple&lt;/code&gt; object for each set of parameters, which helps simplify the code&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I also organized hyperparameters into different classes, which helps when dealing with a large number of hyperparameters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;h3 id=&#34;methods-used-in-the-solution-we-submitted&#34;&gt;Methods used in the solution we submitted&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This section covers the approach we ended up using in the winning solution&lt;/li&gt;
&lt;li&gt;There were four main categories of methods we used:&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rl-algorithm&#34;&gt;RL algorithm&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We used 
&lt;a href=&#34;https://arxiv.org/abs/2009.04416&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phasic Policy Gradient (PPG)&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;PPG is basically an extension of 
&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPO&lt;/a&gt; that&amp;rsquo;s more sample efficient
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s more sample efficient because it performs more updates with a given batch of data than PPO (though only to the value function), and because it avoids or minimizes gradient interference between the gradients from the policy and value objectives&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;See the paper for details since it&amp;rsquo;s presented clearly there&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We used the single-network variant of PPG to improve computational efficiency
&lt;ul&gt;
&lt;li&gt;This is because of the 2-hour time limit for training&lt;/li&gt;
&lt;li&gt;In the paper, they detach the value head (during policy updates) when using a single network, but we found this resulted in some instability (e.g., the network would collapse in some envs)
&lt;ul&gt;
&lt;li&gt;They mention they were worried about this in the paper, but that it wasn&amp;rsquo;t a problem for them&lt;/li&gt;
&lt;li&gt;My guess is that changes to other hyperparameters resulted in it causing problems for us&lt;/li&gt;
&lt;li&gt;Instead of detaching the value head, we just reduced the value loss coefficient&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We 
&lt;a href=&#34;https://github.com/wulfebw/neurips2020-procgen/blob/master/algorithms/data_augmenting_ppo_agent/sync_phasic_optimizer.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implemented&lt;/a&gt; PPG in rllib by implementing a new &lt;code&gt;PolicyOptimizer&lt;/code&gt; subclass
&lt;ul&gt;
&lt;li&gt;It just stores the states and value targets in a buffer, and runs the value / auxiliary epochs occasionally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data-augmentation&#34;&gt;Data augmentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We originally used data augmentation to improve generalization performance, but it turned out that it also improved sample efficiency
&lt;ul&gt;
&lt;li&gt;I believe the reason for this is that the data augmentation prevents overfitting to the levels observed so far&lt;/li&gt;
&lt;li&gt;This results in the policy generalizing better to unseen levels it encounters during training&lt;/li&gt;
&lt;li&gt;As a result, it performs better in the environment more quickly, which allows it to learn about the optimal policy more quickly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We tried the methods from this 
&lt;a href=&#34;https://arxiv.org/abs/2004.14990&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Here&amp;rsquo;s a graphic from the paper showing the effect of the augmentations
&lt;figure class=img-fig&gt;
  &lt;img src=&#34;augmentations.png&#34; alt=&#34;grayscale&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;We ultimately ended up using the random translation augmentation
&lt;ul&gt;
&lt;li&gt;This worked well in most environments&lt;/li&gt;
&lt;li&gt;Some augmentations worked better in certain environments, but would hurt performance in others&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The random translation code need to run quickly due to the 2-hour time limit, so I ended up implementing it manually instead of using &lt;code&gt;numpy.roll&lt;/code&gt; or something similar
&lt;ul&gt;
&lt;li&gt;The implementation is pretty ugly, so I won&amp;rsquo;t include it (see 
&lt;a href=&#34;https://github.com/wulfebw/neurips2020-procgen/blob/master/algorithms/data_augmentation/data_augmentation.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;), but it ended up being about 4x faster than &lt;code&gt;numpy.roll&lt;/code&gt; (there might be a more suitable function in numpy, but couldn&amp;rsquo;t find one)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rl-tricks&#34;&gt;RL tricks&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We tried the RL tricks from this 
&lt;a href=&#34;https://arxiv.org/abs/2005.12729&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;That paper focuses on continuous control tasks, but we figured the results would likely transfer&lt;/li&gt;
&lt;li&gt;They found that reward normalization was the most important implementation detail&lt;/li&gt;
&lt;li&gt;And that&amp;rsquo;s also what we found, and ended up using only that change&lt;/li&gt;
&lt;li&gt;In the paper, they discuss the options for normalizing rewards
&lt;ul&gt;
&lt;li&gt;The most obvious thing to do is to normalize the reward values themselves by subtracting their mean and dividing by their standard deviation
&lt;ul&gt;
&lt;li&gt;The main issue with this is that the goal is for the target-being-predicted (i.e., the state-value) to be normalized
&lt;ul&gt;
&lt;li&gt;But normalizing the rewards individually doesn&amp;rsquo;t accomplish this
&lt;ul&gt;
&lt;li&gt;For example, if you assume the normalized rewards are distributed normally with zero mean and unit variance, then their sum will be normally distributed with zero mean, but with variance 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;the sum of the individual variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;So instead, you would ideally normalize the returns using their mean / variance
&lt;ul&gt;
&lt;li&gt;This is approximately the approach taken by the 
&lt;a href=&#34;https://github.com/openai/phasic-policy-gradient/blob/master/phasic_policy_gradient/reward_normalizer.py#L59&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPG paper&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;They do something where they reverse-discount the observed rewards (see the link for an explanation), which I don&amp;rsquo;t fully understand&lt;/li&gt;
&lt;li&gt;I just computed the variance of the discounted returns, and divided by that ignoring the discount factor
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;m not sure why they take their approach instead of just doing this&lt;/li&gt;
&lt;li&gt;One reason might be that early on (in the rollouts) you don&amp;rsquo;t know what the returns are, so how do you normalize by their statistics?
&lt;ul&gt;
&lt;li&gt;I just ignored this problem, but it&amp;rsquo;s possible their approach addresses it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The second issue is that you generally do not want to change the mean of reward values in reinforcement learning
&lt;ul&gt;
&lt;li&gt;The justification being that (mean-normalizing rewards), 
&lt;a href=&#34;http://joschu.net/docs/nuts-and-bolts.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;affects the agent&amp;rsquo;s will to live&amp;rdquo;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;For example, if all the rewards are initially positive, then an agent is incentivized to avoid terminal / absorbing states&lt;/li&gt;
&lt;li&gt;But if you mean-subtract the rewards, then the agent may be incentivized to seek out terminal / absorbing states (depending on whether it&amp;rsquo;s encountering negative rewards)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;So ultimately I ended up with something along the lines of:
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def normalize_rewards_running_return(policy, rewards, eps=1e-8):
    &amp;quot;&amp;quot;&amp;quot;Updates reward statistics, and normalizes rewards by their standard deviation.
            
        Args:
            policy: The policy being optimized
            rewards: Numpy array of reward values
            eps: Value to add to denominator to avoid numerical issues
              
        Returns:
            Normalized rewards
    &amp;quot;&amp;quot;&amp;quot;
    # Implements Welford&#39;s Algorithm (https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance)
    policy.reward_stats.update(rewards)
    return rewards / (policy.reward_stats.std + eps)
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;The OpenAI 
&lt;a href=&#34;https://github.com/openai/phasic-policy-gradient/blob/master/phasic_policy_gradient/reward_normalizer.py#L59&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;implementation&lt;/a&gt; computes a single variance across workers
&lt;ul&gt;
&lt;li&gt;I just ignored this aspect and it seemed to work fine&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second RL trick we used was disincentivizing no-op actions&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A lot of the environments in Procgen have no-op actions (i.e., actions that when taken have no effect)
&lt;ul&gt;
&lt;li&gt;Since the agent is limited to 8 million steps, taking no-op actions makes the agent less sample efficient (it basically wastes samples)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To disincentivize no-op actions, we checked for cases where the observation (i.e., image) didn&amp;rsquo;t change across timesteps
&lt;ul&gt;
&lt;li&gt;You could argue that this is taking advantage of a limitation of the Procgen environments, which is that they don&amp;rsquo;t resemble natural images, which would (likely) never have duplicate images across timesteps
&lt;ul&gt;
&lt;li&gt;In the real-world case, you have to do something more complicated, for example using some more general 
&lt;a href=&#34;http://people.idsia.ch/~juergen/creativity.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;intrinsic motivation method&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;From this perspective, penalizing no-ops is basically an efficient, environment-specific, intrinsic motivation strategy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One environment where it helps a lot is in Miner
&lt;ul&gt;
&lt;li&gt;In this case, the agent learns not to run into walls / bricks that won&amp;rsquo;t move&lt;/li&gt;
&lt;li&gt;But it also learns to rapidly switch between states rather than remain stationary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A problem with this approach is that it doesn&amp;rsquo;t work for environments where the image changes despite no-op actions
&lt;ul&gt;
&lt;li&gt;For example, in Chaser the image changes because the environment has the chasing agents moving around&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hyperparameter-tuning&#34;&gt;Hyperparameter tuning&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Hyperparameters have a large impact on the performance of RL algorithms
&lt;ul&gt;
&lt;li&gt;PPO is actually less sensitive to hyperparameter values than other algorithms like DQN, but they still matter&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tuning hyperparameters properly requires a lot of time and/or resources
&lt;ul&gt;
&lt;li&gt;To reduce the cost, I generally assumed that hyperparameters were independent whenever reasonable&lt;/li&gt;
&lt;li&gt;I also ran hyperparamter tuning throughout the competition, tuning only one or two hyperparameters are once to check whether some newly-implemented method improved performance
&lt;ul&gt;
&lt;li&gt;As a result, the final hyperparameters were likely suboptimal / outdated&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This table gives the values of hyperparameters that differ from the default values from the 
&lt;a href=&#34;https://arxiv.org/abs/1912.01588&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Procgen&lt;/a&gt; paper and that I think are important&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left&#34;&gt;Hyperparameter&lt;/th&gt;
&lt;th style=&#34;text-align:right&#34;&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Network Architecture&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;Impala (32, 48, 64)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Rollout Fragment Length&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of Workers&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Number of Envs per Worker&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Minibatch Size&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;1750&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PPG Aux Loss Every k&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;PPG Aux Loss Number of Epochs&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Value Loss Coefficient&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Framestack&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;Dropout Probability&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left&#34;&gt;No-op Penalty&lt;/td&gt;
&lt;td style=&#34;text-align:right&#34;&gt;-0.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h6 id=&#34;heading&#34;&gt;&lt;/h6&gt;
&lt;ul&gt;
&lt;li&gt;Some comments on these hyperparameter values:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;number of workers&lt;/strong&gt; and &lt;strong&gt;number of envs per worker&lt;/strong&gt; were set to minimize training wall clock time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;rollout fragment length&lt;/strong&gt; was set to a much smaller value than that of the original paper (16 versus 256)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The reason being that I wanted the data in each minibatch to be as uncorrelated as possible, and having a small rollout fragment length allowed for running a large number of environments in parallel, which results in less correlated data&lt;/li&gt;
&lt;li&gt;This mainly seemed to improve consistency of the algorithm, but it also improved performance a great deal on one of the private environments (graph downloaded from submission (since the environment is private), but it&amp;rsquo;s from the &lt;code&gt;Safezone&lt;/code&gt; environment, which other teams reported difficulty with):
  &lt;figure class=img-fig&gt;
    &lt;img src=&#34;safezone.png&#34; alt=&#34;grayscale&#34; class=&#34;img-sm&#34;/&gt;
  &lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;network size&lt;/strong&gt; and &lt;strong&gt;minibatch size&lt;/strong&gt; were tuned jointly over the course of the competition to balance training time and performance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;value loss coefficient&lt;/strong&gt; was set lower than typical to account for not detaching the value heading (as discussed in the PPG section)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using a nonzero &lt;strong&gt;dropout probability&lt;/strong&gt; improved performance (surprisingly, because dropout typically slows down training a lot)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The main reason I can think of for why this would be the case is that might improve generalization to new levels sufficiently to compensate for the increase in training time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-methods-we-tried-that-didnt-improve-performance&#34;&gt;Other methods we tried that didn&amp;rsquo;t improve performance&lt;/h3&gt;
&lt;h4 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We also tried DQN / 
&lt;a href=&#34;https://arxiv.org/abs/1710.02298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rainbow&lt;/a&gt;, and 
&lt;a href=&#34;https://arxiv.org/pdf/1801.01290.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Soft Actor-Critic&lt;/a&gt;, and couldn&amp;rsquo;t get either to perform better
&lt;ul&gt;
&lt;li&gt;These methods should in theory be more sample efficient because they can reuse the data to a larger extent
&lt;ul&gt;
&lt;li&gt;This is because both are off-policy methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rainbow also implements a variety of methods that should improve performance
&lt;ul&gt;
&lt;li&gt;For example, 
&lt;a href=&#34;https://arxiv.org/abs/1706.10295&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noisy Nets&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/abs/1511.05952&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prioritized experience replay&lt;/a&gt;, 
&lt;a href=&#34;https://arxiv.org/abs/1707.06887&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;distributional RL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We got DQN to work on some envs, it just required a lot more experience / very slow training (e.g., large target network update interval, small learning rate, etc)
&lt;ul&gt;
&lt;li&gt;This matches the result from the Procgen paper
&lt;ul&gt;
&lt;li&gt;They give a possible explanation for DQN&amp;rsquo;s poor performance
&lt;ul&gt;
&lt;li&gt;Which is basically that the diversity of the levels for each environment gives DQN trouble&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Why DQN has more trouble with diverse environments than e.g., PPO I&amp;rsquo;m not sure&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I think that if you removed the time constraint (two hours) on the training time (while still constraining to 8 million time steps), these methods should be able to achieve better performance
&lt;ul&gt;
&lt;li&gt;But given the time constraint, PPG performed better because it already reuses the data to the maximum extent possible (approximately)
&lt;ul&gt;
&lt;li&gt;This is the main advantage of PPG: that it gives a way to make PPO more sample efficient through greater sample reuse&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;exploration&#34;&gt;Exploration&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In many of the environments, dithering exploration strategies are quite poor&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For example, in the Heist environment, a dithering exploration strategy takes a very long time to solve many levels&lt;/li&gt;
&lt;li&gt;Another example is that in many levels, a single random action can result in the agent being killed
&lt;ul&gt;
&lt;li&gt;e.g., Chaser, Miner, Leaper, etc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We implemented 
&lt;a href=&#34;https://arxiv.org/abs/1706.10295&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Noisy Nets&lt;/a&gt;, but for the policy gradient case&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This poses a few challenges for PPO
&lt;ul&gt;
&lt;li&gt;For example, adding noise to the policy output when sampling results in the training policy being different from the sampling policy
&lt;ul&gt;
&lt;li&gt;Since the PPO update implicitly constrains the policy to not change to much, the additional change due to the noise makes the policy update less efficient&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You can get around this issue somewhat by maintaining the same noise vector during sampling as during updating
&lt;ul&gt;
&lt;li&gt;The problems with this are that maintaining that noise vector is fairly complicated in RLLib, but more importantly with noisy nets that means different samples in each batch might have different fully connected weights
&lt;ul&gt;
&lt;li&gt;And running a forward pass with different weights is extremely inefficient (depending somewhat on which weights change)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;As a result of those challenges, we couldn&amp;rsquo;t get noisy nets to improve performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An alternative to Noisy Nets for improving exploration that uses approximately the same intuition (that dithering strategies are worse than temporally-coherent exploration) that works better with PPO is 
&lt;a href=&#34;https://arxiv.org/pdf/1807.10299.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;variational options&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;These methods basically all introduce some sort of latent variable as input to the policy&lt;/li&gt;
&lt;li&gt;And the policy is rewarded for having the latent variable have high mutual information with the future state-action pair(s)&lt;/li&gt;
&lt;li&gt;The result is that you can reduce dithering in the policy (e.g., by reducing or removing the entropy bonus), and only rely on the noise in the latent variable sampling to provide exploration
&lt;ul&gt;
&lt;li&gt;The problem with this for PPO/PPG is, first, that the entropy bonus serves to make the policy update more stable (so getting rid of it makes the algorithm less stable)&lt;/li&gt;
&lt;li&gt;And, second, it doesn&amp;rsquo;t make sense to deterministically select actions for PPO when sampling rollouts
&lt;ul&gt;
&lt;li&gt;The result is that you keep the random action sampling, which diminishes the effect of the variational option because you&amp;rsquo;re still employing dithering exploration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;form-of-the-policy--model&#34;&gt;Form of the policy / model&lt;/h4&gt;
&lt;h5 id=&#34;accounting-for-partial-observability&#34;&gt;Accounting for partial observability&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Some of the environments are partially observable even when provided with the last ~4 frames
&lt;ul&gt;
&lt;li&gt;For example, Jumper requires remembering when you last jumped&lt;/li&gt;
&lt;li&gt;Another example is that caveflyer may require you to remember where you&amp;rsquo;ve already explored for the exit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To account for this partial observability, we tried out a recurrent policy
&lt;ul&gt;
&lt;li&gt;This slowed down learning relative to the CNN policy to the extent that accounting for the partial observability wasn&amp;rsquo;t worth it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;object-based-inductive-bias&#34;&gt;Object-based inductive bias&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;An idea we tried out early on was to encode an object-based inductive bias
&lt;ul&gt;
&lt;li&gt;For example, we tried having the model feature extraction use the model from 
&lt;a href=&#34;https://arxiv.org/abs/1901.11390&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MONet&lt;/a&gt; or 
&lt;a href=&#34;https://arxiv.org/abs/1911.12247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C-SWM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The intuition behind this is that, all the environments involve a relatively small set of interacting objects (small relative to number of pixels)
&lt;ul&gt;
&lt;li&gt;If we can represent the state as a set of objects and relationships between them instead of as pixels, we should be able to learn more efficiently
&lt;ul&gt;
&lt;li&gt;Because that object-based representation of the state is much lower-dimensional&lt;/li&gt;
&lt;li&gt;Because the object-based representation encodes some invariances that don&amp;rsquo;t exist in the image representation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This didn&amp;rsquo;t work because the object representations that were learned didn&amp;rsquo;t correspond to the objects relevant for planning
&lt;ul&gt;
&lt;li&gt;For example, the &amp;ldquo;objects&amp;rdquo; discovered by these algorithms were generally the foreground and background of the image instead of e.g., the different agents in the scene
&lt;ul&gt;
&lt;li&gt;This makes sense when using an objective based on e.g., reconstructing the observation and not on planning performance&lt;/li&gt;
&lt;li&gt;But even when using the RL objective exclusively, the object representations learned weren&amp;rsquo;t useful&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;data-augmentation-1&#34;&gt;Data augmentation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;We tried automatically deciding which data augmentation to use using a multi-arm bandit algorithm
&lt;ul&gt;
&lt;li&gt;This approach was proposed in this 
&lt;a href=&#34;https://arxiv.org/abs/2006.12862&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The approach worked in the paper when they were using 25 million steps, but we found that using 8 million steps wasn&amp;rsquo;t enough to consistently find the best augmentation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We also tried 
&lt;a href=&#34;https://arxiv.org/abs/2010.10814&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mixreg&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;This method augments the data by forming a convex combinations of observations and their outputs
&lt;ul&gt;
&lt;li&gt;Here&amp;rsquo;s an example taken from the paper:
&lt;figure class=img-fig&gt;
    &lt;img src=&#34;mixreg.png&#34; alt=&#34;grayscale&#34; class=&#34;img-sm&#34;/&gt;
&lt;/figure&gt;
&lt;/li&gt;
&lt;li&gt;The intuition being that it should result in augmented data that more closely resembles unseen states than other augmentation methods&lt;/li&gt;
&lt;li&gt;We found that it didn&amp;rsquo;t work as well as just random translation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lessons&#34;&gt;Lessons&lt;/h2&gt;
&lt;p&gt;This section covers some miscellaneous lessons learned&lt;/p&gt;
&lt;h3 id=&#34;about-rl-competition-strategy&#34;&gt;About RL competition strategy&lt;/h3&gt;
&lt;h4 id=&#34;1-avoid-overfitting-to-the-leaderboard-when-its-not-the-test-set&#34;&gt;1. Avoid overfitting to the leaderboard (when it&amp;rsquo;s not the test set)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;At the end of the competition I was 7th on the leaderboard, but ended up getting 1st
&lt;ul&gt;
&lt;li&gt;The leaderboard was determined based on six public environments, whereas the final ranking was based on the full set of sixteen public environments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I suspect part of the reason for this was that some of the other teams were overfitting to the leaderboard (e.g., choosing their algorithm based only on the environments considered by the leaderboard)
&lt;ul&gt;
&lt;li&gt;This is a reasonable strategy if you don&amp;rsquo;t have much compute resources to use to evaluate performance across all environments, but should be avoided if possible&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-optimize-for-performance-on-private-environments&#34;&gt;2. Optimize for performance on private environments&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The scoring for the competition was &lt;code&gt;0.5 * public score + 0.5 private score&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The public score was based on 16 envs, whereas the private score was based on 4 envs&lt;/li&gt;
&lt;li&gt;So everything else being equal it&amp;rsquo;s better to improve performance on the private envs&lt;/li&gt;
&lt;li&gt;Since the leaderboard leaked information about performance on the private envs this was possible
&lt;ul&gt;
&lt;li&gt;There were a few cases where the best hyperparameters on the public envs were different than on the private envs, and I went with the values best on the private envs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;about-rl-competitions&#34;&gt;About RL competitions&lt;/h3&gt;
&lt;h4 id=&#34;1-rl-competitions-are-largely-about-engineering-as-opposed-to-performing-novel-research&#34;&gt;1. RL competitions are largely about engineering as opposed to performing novel research&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;There is a huge design space for RL algorithms
&lt;ul&gt;
&lt;li&gt;By this I mean that a large number of methods exist, and it&amp;rsquo;s unclear without trying them out what will work best&lt;/li&gt;
&lt;li&gt;Just implementing these existing methods is (a) time consuming and (b) likely to yield &amp;ldquo;state-of-the-art&amp;rdquo; performance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Implementing existing algorithms is also much lower risk than trying to develop a new algorithm with respect to competition performance
&lt;ul&gt;
&lt;li&gt;Highly-novel algorithms often require a few iterations of improvements before they outperform existing methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RL algorithms can be heavily compute-optimized, and RL competitions often require some computational limitations
&lt;ul&gt;
&lt;li&gt;There&amp;rsquo;s similarly a large design space / number of options to optimize performance (e.g., how samples are stored, lower-precision training, etc)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For those reasons, I&amp;rsquo;m not sure RL competitions are a good way to promote the discovery of novel algorithms
&lt;ul&gt;
&lt;li&gt;Which is largely the motivation for competitions hosted as part of conferences&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Counter arguments:
&lt;ul&gt;
&lt;li&gt;Supervised learning competitions have been shown to be effective at promoting novel research
&lt;ul&gt;
&lt;li&gt;E.g., ImageNet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s possible there just wasn&amp;rsquo;t enough interest in this competition
&lt;ul&gt;
&lt;li&gt;As a result, novel solutions weren&amp;rsquo;t necessary to win&lt;/li&gt;
&lt;li&gt;But it&amp;rsquo;s possible that if there was greater interest that novel solutions would be required&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;2-rl-competitions-are-a-good-way-to-find-out-how-well-existing-methods-can-solve-a-task&#34;&gt;2. RL competitions are a good way to find out how well existing methods can solve a task&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;While they may not be great at promoting novelty, they are good at finding out how well existing methods can perform
&lt;ul&gt;
&lt;li&gt;This is because they get a lot of people trying out different combinations of existing methods
&lt;ul&gt;
&lt;li&gt;i.e., they help cover the large design space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In the case of the Procgen competition, the performance of the best approaches was better than the baseline performance by a surprising amount&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;3-the-procgen-benchmark-promotes-general-approaches&#34;&gt;3. The Procgen benchmark promotes general approaches&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Early in the competition I tried a number of methods that were motivated by improving performance in a single environment&lt;/li&gt;
&lt;li&gt;For almost all these methods, it turned out that while they improved performance in one environment, they hurt performance in the others&lt;/li&gt;
&lt;li&gt;This makes me think that the diversity of environments in Procgen does a good job of favoring general-purpose methods over environment-specific ones, which I think is an important quality in an RL benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;4-working-with-multiple-people-in-a-competition-has-a-lot-of-advantages&#34;&gt;4. Working with multiple people in a competition has a lot of advantages&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;This one is a bit obvious, but just including it for my future reference&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;appendices&#34;&gt;Appendices&lt;/h2&gt;
&lt;h3 id=&#34;appendix-1-reasons-for-suboptimal-performance&#34;&gt;Appendix 1: Reasons for suboptimal performance&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Here&amp;rsquo;s an outline of reasons the agent might be performing suboptimally.&lt;/li&gt;
&lt;li&gt;This categorization is not perfect:
&lt;ul&gt;
&lt;li&gt;It conflates (i) problems, (ii) root causes of problems, and (iii) solutions to those problems.&lt;/li&gt;
&lt;li&gt;It would be better organized as a graph with cycles instead of as a tree, because different problems can have the same cause.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s a work in progress.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Reinforcement Learning Algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suboptimal exploration
&lt;ul&gt;
&lt;li&gt;Inefficient exploration due to no-op actions&lt;/li&gt;
&lt;li&gt;Lack of entropy regularization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Policy optimization
&lt;ul&gt;
&lt;li&gt;High-variance updates
&lt;ul&gt;
&lt;li&gt;Minibatch contains highly-correlated data
&lt;ul&gt;
&lt;li&gt;Number of parallel envs / works is too small&lt;/li&gt;
&lt;li&gt;Rollout fragment length is too long&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Minibatch size too small&lt;/li&gt;
&lt;li&gt;Discount factor is too large&lt;/li&gt;
&lt;li&gt;Lack of reward normalization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;High-bias updates
&lt;ul&gt;
&lt;li&gt;Rollout fragment length is too short&lt;/li&gt;
&lt;li&gt;Bootstrap value function estimate poor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimization objective doesn&amp;rsquo;t reflect true objective
&lt;ul&gt;
&lt;li&gt;Entropy regularization is too strong&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Value optimization
&lt;ul&gt;
&lt;li&gt;target values are not normalized
&lt;ul&gt;
&lt;li&gt;Due to a lack of reward normalization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Model&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Forgets previously-learned information, or learns slowly
&lt;ul&gt;
&lt;li&gt;Limited model capacity&lt;/li&gt;
&lt;li&gt;Large weight updates
&lt;ul&gt;
&lt;li&gt;Lack of gradient clipping&lt;/li&gt;
&lt;li&gt;Lack of reward normalization&lt;/li&gt;
&lt;li&gt;Due to rare events in the environment with high reward value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gradient interference
&lt;ul&gt;
&lt;li&gt;Sharing policy and value networks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Internal covariate shift
&lt;ul&gt;
&lt;li&gt;Lack of batch normalization&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Insufficient / incorrect inductive bias
&lt;ul&gt;
&lt;li&gt;Image-related issues
&lt;ul&gt;
&lt;li&gt;Overly dependent on texture&lt;/li&gt;
&lt;li&gt;Overly dependent on color&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Lack of invariance to reasonable transforms
&lt;ul&gt;
&lt;li&gt;Poor or insufficient data augmentation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Partial observability
&lt;ul&gt;
&lt;li&gt;Lack of sufficient observation history
&lt;ul&gt;
&lt;li&gt;Frame stacking insufficient&lt;/li&gt;
&lt;li&gt;Form of the model insufficient (e.g., should use RNN instead)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Previous action required&lt;/li&gt;
&lt;li&gt;Timestep required&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Optimization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suboptimal optimizer
&lt;ul&gt;
&lt;li&gt;SGD variants&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Poor gradient clipping
&lt;ul&gt;
&lt;li&gt;Clipping by global norm slows down learning too much&lt;/li&gt;
&lt;li&gt;Clipping elementwise prevents learning optimal behavior&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning rate is too high&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generalization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lack of invariance to irrelevant changes&lt;/li&gt;
&lt;li&gt;Invariance too strong&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Super Smash Bros. 64 AI: Reinforcement Learning Part 1: The Environment</title>
      <link>https://wulfebw.github.io/post/ssb64-rl-01/</link>
      <pubDate>Fri, 05 Jun 2020 22:14:38 -0700</pubDate>
      <guid>https://wulfebw.github.io/post/ssb64-rl-01/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The existing Nintendo 64 Reinforcement learning environments have some disadvantages. 
&lt;a href=&#34;https://github.com/openai/retro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gym-Retro&lt;/a&gt; can almost support N64 emulation, but not quite. I made some changes to the Gym-Retro code to allow for N64 environments. This post describes some of the challenges in doing that, initial results in training an agent in Super Smash Bros. 64, and next steps.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can find the code 
&lt;a href=&#34;https://github.com/wulfebw/retro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Here&amp;rsquo;s an example of an RL agent (Pikachu) playing the game AI (level 9 DK):&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/xlh6xhtkBRY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;environment&#34;&gt;Environment&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;There are some existing N64 RL environments. 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gym-Mupen64Plus&lt;/a&gt; is the main one. It works by running an N64 emulator, taking screenshots of the game, and parsing the screen to compute rewards. This has some issues. First, it means that the environment runs at a variable rate with respect to the agent (i.e., the time between taking an action and having it enacted in the environment is variable). Second, parsing the screen is brittle and not scalable (to more games) compared with reading values from RAM. Third, the screen-shotting mechanism has some disadvantages, for example it&amp;rsquo;s difficult to run multiple environments at once.&lt;/p&gt;
&lt;p&gt;The goal of this project was to implement an RL environment for N64 that didn&amp;rsquo;t have these issues, and that operated like a typical gym environment.&lt;/p&gt;
&lt;h3 id=&#34;challenges&#34;&gt;Challenges&lt;/h3&gt;
&lt;p&gt;When I started, I considered a few options for implementing the environment, and settled on adapting the 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gym-Retro&lt;/a&gt; codebase to allow for N64 emulation. I wasn&amp;rsquo;t sure when I started why Gym-Retro didn&amp;rsquo;t already support N64 games. It turns out the main reasons for that are (i) N64 uses dynamic memory locations for game data, and Gym-Retro is implemented assuming static memory locations and (ii) emulating N64 visuals requires adding OpenGL support (and even with this support emulation is fairly slow).&lt;/p&gt;
&lt;h4 id=&#34;n64-emulation&#34;&gt;N64 Emulation&lt;/h4&gt;
&lt;p&gt;Gym-Retro incorporates the console emulators (called &amp;ldquo;cores&amp;rdquo;, borrowing 
&lt;a href=&#34;https://www.libretro.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;libretro&lt;/a&gt; terminology) as subrepos using 
&lt;a href=&#34;https://github.com/ingydotnet/git-subrepo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git-subrepo&lt;/a&gt;. I took this same approach, and added 
&lt;a href=&#34;https://github.com/libretro/mupen64plus-libretro-nx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mupen64Plus-Next&lt;/a&gt;. This seemed to be the most actively developed libretro N64 core.&lt;/p&gt;
&lt;h4 id=&#34;n64-dynamic-memory&#34;&gt;N64 dynamic memory&lt;/h4&gt;
&lt;p&gt;N64 games (at least Super Smash Bros. 64) store data in memory in dynamic locations, but the address of that data is stored in a constant location. For example, the health of player one might be stored (approximately) anywhere in the block of RAM from &lt;code&gt;0x80000000&lt;/code&gt; to &lt;code&gt;0x88000000&lt;/code&gt; (see 
&lt;a href=&#34;https://en.wikibooks.org/wiki/N64_Programming/Memory_mapping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;details&lt;/a&gt; on N64 memory map); however, the pointer to this health value is always stored at &lt;code&gt;0x80000000&lt;/code&gt; + &lt;code&gt;0x000A50E8&lt;/code&gt; + &lt;code&gt;0x00000020&lt;/code&gt; + &lt;code&gt;0x0000004C&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Gym-Retro isn&amp;rsquo;t designed for this sort of dynamic memory lookup, which is (I believe) partially why N64 wasn&amp;rsquo;t supported. In order to handle this, I (approximately) copied the approach taken by 
&lt;a href=&#34;http://tasvideos.org/BizHawk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BizHawk&lt;/a&gt; of having a separate memory perform the data lookup, but implemented this in python instead of c++. One subtle aspect to this is that the data pointers refer to absolute locations in memory, but the memory available is just the RDRAM section of size &lt;code&gt;0x08000000&lt;/code&gt;. So reading a location in memory ends up looking like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Skipping the rest of the class...
def read_address(self, ptr):
    self.assert_valid_ram_address(ptr)
    addr_list = self.ram[ptr:ptr + self.addr_size]
    abs_addr = convert_byte_list_to_int(addr_list)
    rel_addr = abs_addr - self.rambase
    self.assert_valid_ram_address(rel_addr)
    return rel_addr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I copied this approach from 
&lt;a href=&#34;https://github.com/Isotarge/ScriptHawk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ScriptHawk&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;opengl&#34;&gt;OpenGL&lt;/h4&gt;
&lt;p&gt;The second challenge in supporting N64 in Gym-Retro is that the N64 libretro core requires that the libretro frontend provide OpenGL support. This requires some effort to implement, but it also means that the environment will be fairly slow to simulate because rendering 3d graphics (even using OpenGL) is slower than rendering the 2d graphics used in the supported cores (like Atari). I know next to nothing about OpenGL, so I basically copied the minimal libretro frontend code from 
&lt;a href=&#34;https://github.com/heuripedes/sdlarch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sdlarch&lt;/a&gt; into the Gym-Retro emulator class (though adapted it to use 
&lt;a href=&#34;https://www.glfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GLFW&lt;/a&gt; instead of 
&lt;a href=&#34;https://www.libsdl.org/release/SDL-1.2.15/docs/html/guidevideoopengl.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDL&lt;/a&gt; because GLFW provided all the functionality I needed and is a lot smaller and could be directly incorporated into the project as a subrepo).&lt;/p&gt;
&lt;p&gt;Even though I just copied the sdlarch code into the emulator, it took some effort to figure out where everything should go. I couldn&amp;rsquo;t find much documentation on how to implement a libretro frontend with OpenGL support online.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The end result is an N64 RL environment using Gym-Retro. Running 16 environments in parallel (on 8 cores) I&amp;rsquo;m able to get 45 frames per second. This is much slower than, for example, 
&lt;a href=&#34;https://github.com/openai/procgen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;procgen&lt;/a&gt;, which can apparently simulate &amp;ldquo;thousands of steps per second on a single core&amp;rdquo;. This might be part of the reason OpenAI moved away from retro to procgen (in addition to the generalization properties of procgen).&lt;/p&gt;
&lt;h2 id=&#34;initial-experiments&#34;&gt;Initial Experiments&lt;/h2&gt;
&lt;p&gt;After getting the environment running I ran some initial experiments to make sure it works. These are &amp;ldquo;initial&amp;rdquo; in that they involve training against the game AI instead of self-play.&lt;/p&gt;
&lt;h3 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h3&gt;
&lt;p&gt;For these initial experiments, the environment is set up for a single match between Pikachu (the controlled agent) and a level 9 DK. DK acts deterministically (I wasn&amp;rsquo;t sure about this beforehand, but this is clear from the results), though items are included and occur randomly. Since DK&amp;rsquo;s behavior is determined by the items, DK also acts randomly, though in practice this doesn&amp;rsquo;t matter much.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observation space is a stack of two color images. I started with grayscale images, but the agent didn&amp;rsquo;t perform well, I believe because Pikachu blended into the background too much. I think four frames would help since two frames seems not quite Markovian, but using four color frames slowed down training a fair amount. I also used color images because they might make it easier to identify items.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
  &lt;img src=&#34;grayscale.png&#34; alt=&#34;grayscale&#34;/&gt;
  &lt;figcaption&gt;Where&#39;s Pikachu?&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Action space is a multi-discrete space: two dimensions for directional and one for the button press. I could have alternatively used a multi-class action space, but it would have come out to be size 36, which seemed fairly large.&lt;/li&gt;
&lt;li&gt;Reward Function is +1 for killing DK, -1 for dying, +damage dealt / 100. I initially started with -damage received / 100, but that resulted in too much avoidance behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I chose DK as the opponent because the goal is ultimately to play the trained AI, and I&amp;rsquo;d prefer to play with DK. It turns out this isn&amp;rsquo;t a great choice of opponent for a sanity check because DK can take a lot of damage without dying. This results in the agent needing a great deal of exploration to find the value in actually killing DK as opposed to just damaging him a bunch. This is also an issue because the item drops include health, which DK tends to pick up thereby extending the matches.&lt;/p&gt;
&lt;h3 id=&#34;agent&#34;&gt;Agent&lt;/h3&gt;
&lt;h4 id=&#34;reinforcement-learning-algorithm&#34;&gt;Reinforcement Learning Algorithm&lt;/h4&gt;
&lt;p&gt;I started out using the 
&lt;a href=&#34;https://github.com/openai/baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;baselines&lt;/a&gt; implementation of PPO. My plan was originally to compare at least with DQN, but it turns out that because DK acts deterministically (disregarding items) the policy PPO learns is basically a deterministic sequence. For this reason I suspect that 
&lt;a href=&#34;https://arxiv.org/abs/1709.06009&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brute&lt;/a&gt; would probably do better than PPO in this setting. This isn&amp;rsquo;t much of an issue because the real goal is to train an agent through self-play where a deterministic sequence would perform poorly. This uses the 
&lt;a href=&#34;https://arxiv.org/abs/1802.01561&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Impala CNN&lt;/a&gt; because the smaller CNNs didn&amp;rsquo;t work, which is consistent with some of the results on procgen.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;Training for 10 million steps took roughly 3 days on an 8-core machine with an RTX 2070. The plot of average episode reward during training shows fairly consistent improvement, though it trailed off towards the end:&lt;/p&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;img src=&#34;training.png&#34; alt=&#34;training curve&#34; width=&#34;600&#34;/&gt;
&lt;/figure&gt;
&lt;p&gt;Here are two separate matches. In both cases, Pikachu executes (what seems to be) an identical sequence of actions up until a certain point, after which it starts to improvise:&lt;/p&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/xlh6xhtkBRY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;figcaption&gt;I think Pikachu might be trying to pick up an item here, but is just facing the wrong way.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YhzcUxDBPrE&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;figcaption&gt;This one involved more improvisation. It seems like Pikachu has at least learned to perform an up-B maneuver when off the map.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;h3 id=&#34;environment-1&#34;&gt;Environment&lt;/h3&gt;
&lt;h4 id=&#34;observation-space&#34;&gt;Observation Space&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Vision: I think the visual case is approximately as performant as it&amp;rsquo;s going to get.&lt;/li&gt;
&lt;li&gt;RAM: the environment provides access to ram, but it currently requires writing a memory class for every game. This could be generalized so that the memory locations just need to be specified in a text file.&lt;/li&gt;
&lt;li&gt;Manual feature sets: 
&lt;a href=&#34;https://openai.com/projects/five/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI 5&lt;/a&gt; and 
&lt;a href=&#34;https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaStar&lt;/a&gt; operate on manually designed sets of features derived from RAM. It&amp;rsquo;s likely that if the goal is to build an RL agent for integration into SSB64 Mods that this is the approach that should be taken because training will be faster and deployment will be lighter-weight. Implementing this would require running the emulator without doing any screen rendering, which would likely require writing a mupen64plus visualization plugin that just does nothing. I suspect that this will be the only way to get good self-play agents working because rendering the screen is too slow and I don&amp;rsquo;t have that much computational resources for doing this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;action-space&#34;&gt;Action Space&lt;/h4&gt;
&lt;p&gt;A comparison of the multi-discrete action space with the multi-class action space might be worthwhile. Multi-class for example performed better in the 
&lt;a href=&#34;https://wulfebw.github.io/post/ssb64-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;behavioral cloning case&lt;/a&gt;, but that&amp;rsquo;s a somewhat large action space to use for reinforcement learning, which might be slower to learn than the multi-discrete option.&lt;/p&gt;
&lt;h4 id=&#34;games&#34;&gt;Games&lt;/h4&gt;
&lt;p&gt;The environment should work for all N64 games, though I believe each will require its own RAM specifications. Those are largely available via 
&lt;a href=&#34;https://github.com/Isotarge/ScriptHawk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ScriptHawk&lt;/a&gt;, though the values there differed from what I found in some cases.&lt;/p&gt;
&lt;h3 id=&#34;agent-1&#34;&gt;Agent&lt;/h3&gt;
&lt;p&gt;A Recurrent policy might be worth trying. There are also questions about how the policy will work in the self-play setting. For example, how will it know which agent it is controlling? Chances are it won&amp;rsquo;t be able to based on the screen alone and some additional information will have to be provided via the RAM.&lt;/p&gt;
&lt;p&gt;A comparison between DQN, MuZero, and other model-based methods could be interesting. DQN might be more sample efficient, and that would help because the interaction here is the bottleneck. MuZero could be interesting because this is effectively a competitive environment, and explicitly modeling it as one might improve performance. A model-based approach using 
&lt;a href=&#34;https://arxiv.org/abs/1911.12247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contrasitve Learning of Structured World Models&lt;/a&gt; (C-SWM) could help improve sample efficiency via an object-based inductive bias. I tried this out on the behavioral cloning dataset and it seems to give some interesting results:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;ssb64-cswm.gif&#34; alt=&#34;ssb64-cswm&#34; width=&#34;200&#34;/&gt;
    &lt;figcaption&gt;C-SWM applied to a SSB64 dataset.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It seems to separate out the foreground from the background, but not the agents. I think correcting for the camera movement could fix that. Learning the model for planning purposes like in MuZero might also put greater emphasis on the agents.&lt;/p&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;The goal is to be able to play an agent trained through self-play. You&amp;rsquo;d ideally be able to use a controller. So I&amp;rsquo;ll need to write a player agent that takes input from a controller and passes that to the gym environment. This shouldn&amp;rsquo;t be too hard using the inputs 
&lt;a href=&#34;https://github.com/zeth/inputs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;lessons&#34;&gt;Lessons&lt;/h2&gt;
&lt;h3 id=&#34;gym-retro&#34;&gt;Gym-Retro&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s how this works: the python interface is provided via 
&lt;a href=&#34;https://github.com/pybind/pybind11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pybind11&lt;/a&gt; (see retro.cpp). retro.cpp has an Emulator object defined in emulator.h/cpp. The emulator class is actually a libretro frontend, though it only implements some of the frontend functionality. This class interacts with libretro cores by dynamically loading functions from them. The integration UI is a UI on top of the emulator with some search functionality. The search functionality doesn&amp;rsquo;t work with N64 due to its use of dynamic memory locations.&lt;/p&gt;
&lt;h3 id=&#34;bugs--things-that-took-a-long-time-to-figure-out&#34;&gt;Bugs / things that took a long time to figure out&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One bug I spent a long time on before I realized that N64 used dynamic memory locations was getting the search functionality in the integration UI to work. There was a bug where the rambase was loaded from a file as a signed int, which when converted to a size_t gave an incorrect value, but only if the rambase was large enough. Probably spent 2 hours on that.&lt;/li&gt;
&lt;li&gt;I typically use pytorch, but was using tensorflow since baselines uses it. Instead of using baselines&#39; &lt;code&gt;run&lt;/code&gt; function I just implemented my own in a script, but didn&amp;rsquo;t look at &lt;code&gt;run&lt;/code&gt; carefully and missed the part where they set a specific tensorflow gpu config. 
&lt;a href=&#34;https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apparently&lt;/a&gt; tensorflow expands to use all (?) of the gpu memory. This resulted in mysterious segfaults from OpenGL that were difficult to debug.&lt;/li&gt;
&lt;li&gt;Figuring out the format of the &lt;code&gt;n64.json&lt;/code&gt; file took a long time. I don&amp;rsquo;t think there are docs on how this is defined. So here&amp;rsquo;s what everything is as far as I can tell:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lib&lt;/code&gt;: this should be a prefix of the name of the shared library the core is compiled to. So for instance mupen64plus is compiled to mupen64plus_next_libretro.so. So &lt;code&gt;lib&lt;/code&gt; should be set to &amp;ldquo;mupen64plus_next&amp;rdquo;. Furthermore, after compiling the core each time, you need to copy the shared library to retro/retro/cores. This is done by setup.py initially, but if you recompile you have to do it manually.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: extension of the rom files for this core.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;types&lt;/code&gt;: I believe this is only used by the searching functionality to limit the scope of its search for variables to the specified types.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rambase&lt;/code&gt;: the location in memory where the relevant portion of ram starts. In the n64 this is 0x80000000. This has an effect on how the emulator reads the ram so you have to set it correctly. To find out what value to use, look up the &amp;ldquo;memory map&amp;rdquo; for your core.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keybinds&lt;/code&gt;: these are the keyboard keys used to manually control the environment.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;buttons&lt;/code&gt;: these are the actual button values used to take actions, and (I believe) their order does matter. It needs to match the order used in the core/libretro.h file. So for example, atari2600/libretro.h:178 shows the button ordering. The buttons in this json file need to match that. All the libretro cores implement a similar ordering it seems.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;actions&lt;/code&gt;: each row is a dimension of the action space (in the multi-discrete case). Within each dimension, each element is an action in that dimension. Within each action, the elements are the buttons that result in that action.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What to do about the analog stick in N64? The analog stick of the other cores just provides directionals, but in N64 it provides the magnitude of the directional, and that has an influence on the behavior in the game. I ended up changing the core to map these fine-grain controls to directionals.&lt;/li&gt;
&lt;li&gt;MAX_PLAYERS: The default MAX_PLAYERS value is 2, but for N64 it should be 4. This resulted in the other players executing random commands when running the game and it took me probably an hour to figure out why that was happening.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main tool I used to figure that stuff out was gdb, but debugging a c program called through python, which is not something I realized you could do previously.&lt;/p&gt;
&lt;h3 id=&#34;n64-emulation-visualization-plugins&#34;&gt;N64 Emulation Visualization Plugins&lt;/h3&gt;
&lt;p&gt;When I first got the emulator working and realized that emulator.h/cpp didn&amp;rsquo;t implement the OpenGL functionality I tried to avoid implementing it myself by using software-defined rendering in the emulator. The library for doing this is referred to as angrylion, and it actually works fine, except that it&amp;rsquo;s very slow, running at about 8 fps compared with OpenGL&amp;rsquo;s 45 fps. However, getting an end-to-end system running with angrylion, and then going back to tackle the relatively difficult task (for me) of implementing the OpenGL functionality for libretro frontends was (accidentally) a good strategy because I probably would have given up if I had had to implement the OpenGL stuff to get anything working. This is mainly because with the angrylion version already implemented, I knew what &amp;ldquo;working&amp;rdquo; looked like, and I also was much more familiar with the system by the time I tried to tackle the OpenGL stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MuZero</title>
      <link>https://wulfebw.github.io/post/muzero/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/muzero/</guid>
      <description>&lt;h1 id=&#34;context&#34;&gt;Context&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1911.08265.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MuZero paper.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/wulfebw/muzero&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tabular implementation.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;See that link for code description and some experiments.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;question&#34;&gt;Question&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Why should MuZero be better than other deep RL algorithms (in particular DQN)?&lt;/li&gt;
&lt;li&gt;The paper, 
&lt;a href=&#34;https://arxiv.org/pdf/1906.05243.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;When to use parametric models in reinforcement learning?&amp;quot;&lt;/a&gt;, goes into this question but with an emphasis on a 
&lt;a href=&#34;https://arxiv.org/pdf/1903.00374.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;different&lt;/a&gt; model-based algorithm
&lt;ul&gt;
&lt;li&gt;They define planning in an unusual manner as using &amp;ldquo;more computation without additional data to improve predictions and behavior&amp;rdquo;
&lt;ul&gt;
&lt;li&gt;The &amp;ldquo;usual&amp;rdquo; definition being the use of a (learned) transition and reward model to compute a value function or policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The reasons they give for why parameteric model-based planning might outperform non-parameteric (experience replay) planning are not that clear to me, but may include:
&lt;ol&gt;
&lt;li&gt;Inductive bias of the parameteric model (improving generalization)&lt;/li&gt;
&lt;li&gt;Improved exploration through use of the model&lt;/li&gt;
&lt;li&gt;Improved credit assignment (&amp;ldquo;backward planning&amp;rdquo;)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To answer the question, I think the reason MuZero might outperfom DQN is the second reason: that it exhibits better exploration
&lt;ul&gt;
&lt;li&gt;This isn&amp;rsquo;t the typical kind of improved exploration through model use that you get with e.g., 
&lt;a href=&#34;https://ie.technion.ac.il/~moshet/brafman02a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R-max&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Instead, because the learned value function is approximate, if the learned model is &amp;ldquo;good enough&amp;rdquo;, then you can use additional computation to select better actions in the environment than you would if you behaved greedily with respect to the value function itself.
&lt;ul&gt;
&lt;li&gt;Where &amp;ldquo;additional computation&amp;rdquo; means running MCTS.&lt;/li&gt;
&lt;li&gt;This means that the actions you take will, on average, be better.&lt;/li&gt;
&lt;li&gt;Which intuitively should mean that you&amp;rsquo;ll learn about the optimal policy and its value function more quickly.
&lt;ul&gt;
&lt;li&gt;And reach different parts of the state space that have higher value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Which can be viewed as improved exploration.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This works for the definition of &amp;ldquo;better&amp;rdquo; as &amp;ldquo;more sample efficient&amp;rdquo;
&lt;ul&gt;
&lt;li&gt;But that additional computation takes more resources and time, so if you defined &amp;ldquo;better&amp;rdquo; as the algorithm that provides the best solution given some amount of time, DQN might outperform MuZero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Super Smash Bros. 64 AI: Behavioral Cloning</title>
      <link>https://wulfebw.github.io/post/ssb64-bc/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/ssb64-bc/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: develop a Super Smash Bros. 64 AI that acts based on image input&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: start with behavioral cloning since it&amp;rsquo;s simple&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: Images and action sequences, and training and validation splits available 
&lt;a href=&#34;https://s3.console.aws.amazon.com/s3/buckets/ssb64-data-public/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; (
&lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;s3 bucket&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model and Training&lt;/strong&gt;: Convolutional and recurrent neural networks, implemented in pytorch, code available 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Through 
&lt;a href=&#34;https://github.com/mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gym-mupen64plus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Quantitative gameplay performance (damage, kills, lives lost)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demo&lt;/strong&gt;: See the description of the video for some interesting events&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Usr00SPbRHg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;!-- markdown-toc start - Don&#39;t edit this section. Run M-x markdown-toc-refresh-toc --&gt;
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#related-work&#34;&gt;Related work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#approach&#34;&gt;Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#experiments-and-results&#34;&gt;Experiments and Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-1-dataset-collection&#34;&gt;Appendix 1: Dataset collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-2-imitation-learning&#34;&gt;Appendix 2: Imitation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-3-model-and-training-details&#34;&gt;Appendix 3: Model and training details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4: N64 as an RL environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-5-rnns-why-use-them-how-to-optimize-and-regularize-them&#34;&gt;Appendix 5: RNNs why use them, how to optimize and regularize them&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- markdown-toc end --&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Build a Super Smash Bros. 64 (SSB64) AI agent to play against.&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related work&lt;/h1&gt;
&lt;h2 id=&#34;n64-and-ssb64-ai&#34;&gt;N64 and SSB64 AI&lt;/h2&gt;
&lt;p&gt;The only info on an SSB64 AI I could find online was this cs231n 
&lt;a href=&#34;http://cs231n.stanford.edu/reports/2016/pdfs/113_Report.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt; with associated 
&lt;a href=&#34;https://www.youtube.com/watch?v=c-6XcmM-MSk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;videos&lt;/a&gt;. It&amp;rsquo;s a nice write up and was helpful in doing this project.&lt;/p&gt;
&lt;p&gt;I thought implementing a similar project myself would be worth doing nevertheless because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;They don&amp;rsquo;t provide access to any data or code&lt;/li&gt;
&lt;li&gt;Their agent seems to behave a bit repetitively&lt;/li&gt;
&lt;li&gt;I was curious whether a different action-space or model would work better&lt;/li&gt;
&lt;li&gt;As stated, I wanted to play a better AI myself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/kevinhughes27/TensorKart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorKart&lt;/a&gt; is a project that developed a Mario Kart 64 AI. That project (and associated blog 
&lt;a href=&#34;https://www.kevinhughes.ca/blog/tensor-kart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt;) was also helpful, and provided the technical approach roughly followed in this project.&lt;/p&gt;
&lt;h2 id=&#34;game-ai-in-general&#34;&gt;Game AI in general&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s a ton of work on game playing. Most of it focuses on reinforcement learning (RL). This is because RL / approximate dynamic programming is currently the best approach to the problem. See for example 
&lt;a href=&#34;https://openai.com/five/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI five&lt;/a&gt; or Deepmind 
&lt;a href=&#34;https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;starcraft II AI &lt;/a&gt; or 
&lt;a href=&#34;https://deepmind.com/research/alphago/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaGo&lt;/a&gt;. RL is the best approach in these game playing scenarios because (among other reasons):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It&amp;rsquo;s possible to collect a large amount of training data through simulation.&lt;/li&gt;
&lt;li&gt;The training and testing distributions closely or exactly match.&lt;/li&gt;
&lt;li&gt;The agent can benefit from 
&lt;a href=&#34;https://openai.com/blog/competitive-self-play/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;competitive self play&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s an easily accessible reward function (e.g., win/lose, damage, deaths) that aligns well with the true goal (developing a competitive agent).&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;p&gt;The dataset consists of a set of matches. See 
&lt;a href=&#34;#appendix-1-dataset-collection&#34;&gt;Appendix 1&lt;/a&gt; for details. Main points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://mupen64plus.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mupen64plus&lt;/a&gt; was used for emulating the N64.&lt;/li&gt;
&lt;li&gt;The dataset consists of a set of gameplay matches.
&lt;ul&gt;
&lt;li&gt;In total about 175,000 frames representing about 6 hours of gameplay.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Matches were played between blue DK and the highest level game AI Kirby.
&lt;ul&gt;
&lt;li&gt;This allows the network to identify which agent it controls.&lt;/li&gt;
&lt;li&gt;Extending this to multiple characters would require e.g., adding a feature indicating the controlled character.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All matches were played on the dreamland stage.&lt;/li&gt;
&lt;li&gt;Items were included to make it more interesting.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&amp;rsquo;s a short clip selected from the dataset to give a sense for the behavior being imitated:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/K4bJlsvCliw&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;p&gt;I started with behavioral cloning (BC), which is supervised learning using a dataset of expert demonstrations, despite it being a worse-performing approach than RL because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It&amp;rsquo;s simpler to implement than RL methods.
&lt;ul&gt;
&lt;li&gt;Allows for becoming familiar with the emulator in a simplified setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It provides a nice baseline for comparison.&lt;/li&gt;
&lt;li&gt;It has more predictable computational costs than RL.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main disadvantages of behavioral cloning are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It may take a lot of data to achieve good performance (see 
&lt;a href=&#34;#appendix-2-imitation-learning&#34;&gt;Appendix 2&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The resulting agent will not perform significantly better than the expert providing the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;It might perform slightly better due to decreased reaction time, but that&amp;rsquo;s not particularly interesting or fun to play against.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;observation-space&#34;&gt;Observation space&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A requirement for this project was that the AI has to act on image observations.
&lt;ul&gt;
&lt;li&gt;There are some alternatives to this observation space (e.g., operating on the game 
&lt;a href=&#34;https://deepsense.ai/playing-atari-on-ram-with-deep-q-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAM&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A single image is not a Markovian representation of the state.
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s unclear to what extent multiple images would improve performance.&lt;/li&gt;
&lt;li&gt;For this reason, different numbers of frames were considered as input (fixed history in the CNN case, and complete history in theory in the RNN case).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;action-space&#34;&gt;Action space&lt;/h2&gt;
&lt;p&gt;I considered two action spaces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exclusive multiclass action space consisting of common SSB64 moves.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This is what the cs231n project uses.&lt;/li&gt;
&lt;li&gt;The actions include  the buttons, the axes, and smash combinations (A + left, A + right, etc).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;A multi-discrete action space with a value for each joystick axis, and one value for the button.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This action space seemed like it should allow for more efficient learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A Resnet-9, multi-frame model.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Using a four-frame history of grayscale images.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;An RNN with Resnet feature extractor.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This takes as input individual grayscale images at each timestep.&lt;/li&gt;
&lt;li&gt;Same visual feature extractor as the fixed-length history model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The models were implemented in pytorch. See 
&lt;a href=&#34;#appendix-3-model-and-training-details&#34;&gt;Appendix 3&lt;/a&gt; for training details.&lt;/p&gt;
&lt;h1 id=&#34;experiments-and-results&#34;&gt;Experiments and Results&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Edit (05/18/2020): I never ran this evaluation because gym-mupen64plus had a number of issues with it
&lt;ul&gt;
&lt;li&gt;See the appendix 4 for more details&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;The models were deployed using 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gym-mupen64plus&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h1&gt;
&lt;p&gt;Despite not being the best approach to the problem, behavioral cloning was a helpful place to start in creating a SSB64 AI. I learned a fair amount about emulating the game (see 
&lt;a href=&#34;#appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4&lt;/a&gt;), 
&lt;a href=&#34;https://github.com/pytorch/ignite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch ignite&lt;/a&gt; (see 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc/blob/master/scripts/train.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;training script&lt;/a&gt;), regularizing RNNs (see 
&lt;a href=&#34;#appendix-5-rnns-why-use-them-how-to-optimize-and-regularize-them&#34;&gt;Appendix 5&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
The main direction for future work is applying RL to the problem. As discussed in 
&lt;a href=&#34;#appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4&lt;/a&gt;, the N64 isn&amp;rsquo;t the most convenient environment for RL for technical reasons, so most of the effort in applying RL would be in creating a convenient environment.&lt;/p&gt;
&lt;h1 id=&#34;appendix-1-dataset-collection&#34;&gt;Appendix 1: Dataset collection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Images were collected by adapting the TensorKart codebase
&lt;ul&gt;
&lt;li&gt;See this script&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I collected the data on a mac. The 
&lt;a href=&#34;https://github.com/zeth/inputs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inputs&lt;/a&gt; python package used in TensorKart doesn&amp;rsquo;t work on mac for joysticks, so I made some changes to the input plugin for mupen64plus to accept a command line argument for a filepath at which to store the actions received. See 
&lt;a href=&#34;https://github.com/wulfebw/mupen64plus-input-sdl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for the fork containing those changes.&lt;/li&gt;
&lt;li&gt;Image-action pairs were created by finding, for each image, the non-null action that occurred most recently following the collected image, up to a maximum of 0.05 seconds after the image, or if no non-null action occurred, a noop action was selected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-2-imitation-learning&#34;&gt;Appendix 2: Imitation Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Some &lt;a href=&#34;https://wulfebw.github.io/assets/imitation_learning_notes.pdf&#34;&gt;notes&lt;/a&gt; on the topic.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-3-model-and-training-details&#34;&gt;Appendix 3: Model and training details&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;https://github.com/wulfebw/ssb64bc/blob/master/scripts/hparams.py&#34;&gt;here&lt;/a&gt; for the hyperparams used in the CNN case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4: N64 as an RL environment&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;mupen64plus seems to be the most popular N64 emulator.&lt;/li&gt;
&lt;li&gt;It is not designed for use in RL.
&lt;ul&gt;
&lt;li&gt;The design is based on a core module that has plugins for video, audio, controller input, etc.&lt;/li&gt;
&lt;li&gt;The plugins don&amp;rsquo;t communicate directly.&lt;/li&gt;
&lt;li&gt;Providing a unified interface for RL would require a significant amount of effort, with the implementation options being:
&lt;ol&gt;
&lt;li&gt;Wrap the core and all plugins in an adapter program.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;And also interpret / provide access to the RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Use 
&lt;a href=&#34;http://tasvideos.org/BizHawk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BizHawk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Bizhawk seems to have already 
&lt;a href=&#34;https://github.com/TASVideos/BizHawk/tree/master/libmupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;done&lt;/a&gt; option 1, and is typically used in tool-assisted SSB64 gameplay.&lt;/li&gt;
&lt;li&gt;It only seems to work on windows for now, with linux support being 
&lt;a href=&#34;https://github.com/TASVideos/BizHawk/issues/1430&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;developed&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Alternatively, you could use 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gym-mupen64plus&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;It takes screenshots of the game and parses them to compute rewards.&lt;/li&gt;
&lt;li&gt;The main problems are:
&lt;ol&gt;
&lt;li&gt;Because the observations are from screenshots, the hardware might influence the rate of observations and their alignment with actions taken.
&lt;ul&gt;
&lt;li&gt;This is complicated to deal with, as can be seen in the dataset 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc/tree/master/ssb64bc/formatting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;formatting details&lt;/a&gt; of this project.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gym-mupen64plus for SSB64 attempts to automatically select characters in versus mode, but that didn&amp;rsquo;t work in my case.
&lt;ul&gt;
&lt;li&gt;The implementation assumes the selector for the AI appears in the same location every time.
&lt;ul&gt;
&lt;li&gt;In my emulation of the game it was random.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Directly performing these tasks by operating on the RAM is clearly a better approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing the screen for the reward seems to work, but breaks with small changes.
&lt;ul&gt;
&lt;li&gt;Using the RAM directly would be faster and more reliable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-5-rnns-why-use-them-how-to-optimize-and-regularize-them&#34;&gt;Appendix 5: RNNs why use them, how to optimize and regularize them&lt;/h1&gt;
&lt;h2 id=&#34;why-use-an-rnn&#34;&gt;Why use an RNN?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I tried using an RNN because I was curious to what extent it would improve performance.
&lt;ul&gt;
&lt;li&gt;The reasons it might improve performance are:
&lt;ol&gt;
&lt;li&gt;A reasonable-size, fixed-frame history may not represent a Markovian representation of the environment.&lt;/li&gt;
&lt;li&gt;The inductive bias of the RNN may provide some benefits (e.g., consistency or smoothness).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;optimizing-and-regularizing-rnns&#34;&gt;Optimizing and regularizing RNNs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First, note that RNNs have gone out of fashion lately (at least in NLP).
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer networks&lt;/a&gt; are generally being preferred.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=Hygxb2CqKm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&amp;rsquo;s&lt;/a&gt; a paper that tries to explain why.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There are a lot of tricks for regularizing and optimizing RNNs.
&lt;ul&gt;
&lt;li&gt;This is a good 
&lt;a href=&#34;https://arxiv.org/pdf/1708.02182.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; on it that is somewhat recent.
&lt;ul&gt;
&lt;li&gt;It focuses on NLP, but the advice seemed to help in my case.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I implemented or copied some of the regularization methods 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc/blob/master/ssb64bc/models/torch_utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One decision to make in RNN training is whether to use &amp;ldquo;stateful&amp;rdquo; or &amp;ldquo;stateless&amp;rdquo; training.
&lt;ul&gt;
&lt;li&gt;See this 
&lt;a href=&#34;https://keras.io/examples/lstm_stateful/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;description&lt;/a&gt; and this 
&lt;a href=&#34;http://philipperemy.github.io/keras-stateful-lstm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explanation&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;The gist is that you need to use stateful (maintaining the hidden state across batches) training if you want the network to learn to maintain memory beyond the length of the BPTT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Agent Imitation Learning for Driving Simulation</title>
      <link>https://wulfebw.github.io/publication/multiagent-gail-driver/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/publication/multiagent-gail-driver/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Two Link Programmatic Control</title>
      <link>https://wulfebw.github.io/post/03-two-link-programmatic-control/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/03-two-link-programmatic-control/</guid>
      <description>&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;If we want to control the robot through imitation learning, we need the ability to manually control it so as to gather data&lt;/li&gt;
&lt;li&gt;Manual control requires programmatic control, i.e., the ability to specify end-effector positions and have the robot achieve those positions through changes in joint configurations&lt;/li&gt;
&lt;li&gt;This post shows how to implement programmatic control of a simple two-link robot&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Programmatic robot control
&lt;ul&gt;
&lt;li&gt;Forward kinematics&lt;/li&gt;
&lt;li&gt;Inverse kinematics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preview&#34;&gt;Preview&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/vHgpbdHIV50&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;materials&#34;&gt;Materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This post only uses materials from previous posts&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;forward-kinematics&#34;&gt;Forward Kinematics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We&amp;rsquo;re focused on robotic manipulators aka robot arms, where the end of the robot arm is called the end-effector&lt;/li&gt;
&lt;li&gt;Given all the information about a robot arm (length and mass of the links, the types of joints, how those joints and links are connected, etc) and the current configuration of the joints, how can you find out where the end-effector is?
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Forward_kinematics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Forward kinematics&lt;/a&gt; answers this question&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;schematic&#34;&gt;Schematic&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;First, draw a schematic of the robot arm, assigning frames to the different joints&lt;/li&gt;
&lt;li&gt;Figure 3 in these 
&lt;a href=&#34;http://www.cs.columbia.edu/~allen/F15/NOTES/jacobians.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notes&lt;/a&gt; provides a schematic for a two-revolute-joint robot (called an revolute-revolute (RR) manipulator)
&lt;ul&gt;
&lt;li&gt;revolute means the joint rotates, and the other option is prismatic meaning it translates linearly along its axis&lt;/li&gt;
&lt;li&gt;Also, here&amp;rsquo;s a schematic with the frames labeled&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;schematic.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;dh-parameters&#34;&gt;DH Parameters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Second, derive 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Denavit%E2%80%93Hartenberg_parameters&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Denavit–Hartenberg parameters&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;DH parameters are one convention for expressing information about robotic manipulators&lt;/li&gt;
&lt;li&gt;They make it easy to derive the transformation from each frame to the end-effector frame&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;transformation-matrix&#34;&gt;Transformation Matrix&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Given the DH parameters, we can derive a transformation matrix from each frame to the frame of the end-effector
&lt;ul&gt;
&lt;li&gt;First, build transformation matrices from each frame to the next&lt;/li&gt;
&lt;li&gt;Second, multiply these all together to find the transformation from the 0th frame to the end-effector frame&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Section 3 of the referenced 
&lt;a href=&#34;http://www.cs.columbia.edu/~allen/F15/NOTES/jacobians.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;notes&lt;/a&gt; provides the transformation matrix for the RR manipulator&lt;/li&gt;
&lt;li&gt;Here&amp;rsquo;s the 
&lt;a href=&#34;https://github.com/wulfebw/robotics_rl/blob/master/rlrobo/dh.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; for computing the transformation matrix given the DH parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inverse-kinematics&#34;&gt;Inverse Kinematics&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We just looked at the forward kinematics, which tell you the position of the end-effector given the configuration of the joints&lt;/li&gt;
&lt;li&gt;What about the opposite question: given a (desired) position of the end-effector how do you figure out the joints that achieve that position?&lt;/li&gt;
&lt;li&gt;This turns out to be a more difficult problem to solve
&lt;ul&gt;
&lt;li&gt;Since the forward kinematics involve nonlinear operations (sine, cosine), typically there&amp;rsquo;s no analytical solution for the joints&lt;/li&gt;
&lt;li&gt;There can be many solutions, and there can also be no solutions&lt;/li&gt;
&lt;li&gt;We&amp;rsquo;ll look at one method for solving the problem&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;jacobian&#34;&gt;Jacobian&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The method we&amp;rsquo;ll use requires the Jacobian&lt;/li&gt;
&lt;li&gt;The forward kinematics map from the joints config to the end-effector position
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s a function mapping from dimension n to dimension m&lt;/li&gt;
&lt;li&gt;If you differentiate each output with respect to each input, and arrange those derivatives in a matrix, that matrix is the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jacobian&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Given the transformation matrices, there&amp;rsquo;s a simple algorithm for finding the Jacobian&lt;/li&gt;
&lt;li&gt;The code implementing the algorithm and along with a description can be found 
&lt;a href=&#34;https://github.com/wulfebw/robotics_rl/blob/master/rlrobo/jacobian.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inverse-jacobian-method&#34;&gt;Inverse Jacobian Method&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The method we use for solving the inverse kinematics problem is called the inverse Jacobian method
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s an iterative method that
&lt;ul&gt;
&lt;li&gt;Linearizes the function using the Jacobian&lt;/li&gt;
&lt;li&gt;Solves for the change in joint configuration that moves the end-effector closest to the target position
&lt;ul&gt;
&lt;li&gt;This is accomplished by formulating the pseudoinverse of the Jacobian, hence the name&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Updates the configuration&lt;/li&gt;
&lt;li&gt;Repeats until the target is reached or it&amp;rsquo;s determined that it cannot be reached&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://inst.eecs.berkeley.edu/~cs184/fa09/resources/ik.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&amp;rsquo;s&lt;/a&gt; a solid tutorial on it that steps through the logic and math&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/wulfebw/robotics_rl/blob/master/rlrobo/inverse_kinematics.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&amp;rsquo;s&lt;/a&gt; my implementation of it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This method does not explicitly deal with constraints on the movement of the manipulator
&lt;ul&gt;
&lt;li&gt;A faster solver that handles constraints will be the topic of a future post&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;walkthrough&#34;&gt;Walkthrough&lt;/h2&gt;
&lt;h3 id=&#34;hardware&#34;&gt;Hardware&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The two-link manipulator is just the connected servos from the previous post, which looks like:
&lt;img src=&#34;manipulator.jpg&#34; alt=&#34;&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;software&#34;&gt;Software&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download and setup the project code&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/wulfebw/robotics_rl.git
cd robotics_rl
sudo python setup.py develop
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Run the tutorial 
&lt;a href=&#34;https://github.com/wulfebw/robotics_rl/blob/master/tutorials/3_two_link_programmatic_control.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; that randomly samples an end-effector position, and then solves for a joint configuration to achieves that position&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time
import rlrobo.manipulator

manipulator = rlrobo.manipulator.build_RR_manipulator(l1=1,l2=2)
while True:
    pos = manipulator.random_position()
    manipulator.set_end_effector_position(pos)
    time.sleep(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/vHgpbdHIV50&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Single Servo Control</title>
      <link>https://wulfebw.github.io/post/02-single-servo-control/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/02-single-servo-control/</guid>
      <description>&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Servos will control the joints of the robotic arm. In this post, we&amp;rsquo;ll look at controlling a single servo using a HAT (hardware attached on top).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preview&#34;&gt;Preview&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/kY3lgvHtju8&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;materials&#34;&gt;Materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;servo HAT
&lt;ul&gt;
&lt;li&gt;$22 on 
&lt;a href=&#34;https://www.amazon.com/gp/product/B00XW2OY5A/ref=oh_aui_detailpage_o01_s01?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;HAT power source
&lt;ul&gt;
&lt;li&gt;$8 on 
&lt;a href=&#34;https://www.amazon.com/gp/product/B00P5P6ZBS/ref=oh_aui_detailpage_o01_s00?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;soldering kit
&lt;ul&gt;
&lt;li&gt;$20 on 
&lt;a href=&#34;https://www.amazon.com/gp/product/B06XZ31W3M/ref=oh_aui_detailpage_o00_s00?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;small test servos
&lt;ul&gt;
&lt;li&gt;$7 on 
&lt;a href=&#34;https://www.amazon.com/gp/product/B013UI9MVG/ref=oh_aui_detailpage_o09_s01?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;total cost: $57 + cost of previous materials
&lt;ul&gt;
&lt;li&gt;cost so far: $147 + cost of common materials&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;h3 id=&#34;how-do-servos-work&#34;&gt;How do servos work?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They have a motor. You send a signal to the motor, and the length of the signal indicates the desired position of the motor
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://learn.sparkfun.com/tutorials/hobby-servo-tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Servo primer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pulse-width_modulation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pulse width modulation (PWM)&lt;/a&gt; is used in sending the signal
&lt;ul&gt;
&lt;li&gt;PWM resources:
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://learn.adafruit.com/adafruits-raspberry-pi-lesson-8-using-a-servo-motor/servo-motors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adafruit description&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;why-do-we-need-the-servo-hat&#34;&gt;Why do we need the servo HAT?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The pi has the ability to send PWM signals, but there are two problems
&lt;ol&gt;
&lt;li&gt;servos draw enough power that operating them can interfere with the pi operations&lt;/li&gt;
&lt;li&gt;the pi only has a single PWM pin, which means it can only control one servo&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;The servo HAT allows for controlling many servos at once, and also uses an external power supply to avoid interfering with the pi&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;walkthrough&#34;&gt;Walkthrough&lt;/h2&gt;
&lt;h3 id=&#34;servo-hat-hardware-setup&#34;&gt;Servo HAT hardware setup&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Solder the pins into the servo HAT then attach the HAT to the pi as in the following picture
&lt;img src=&#34;assembled.jpg&#34; alt=&#34;A connected HAT&#34;&gt;&lt;/li&gt;
&lt;li&gt;Attach the servo to the first set of 3 pins&lt;/li&gt;
&lt;li&gt;Power on the pi and servo HAT&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;servo-hat-software-setup&#34;&gt;Servo HAT software setup&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download the software library that comes with the servo HAT as described in this 
&lt;a href=&#34;https://learn.adafruit.com/adafruit-16-channel-pwm-servo-hat-for-raspberry-pi/using-the-python-library&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Add the downloaded library to PYTHONPATH in your raspberry pi bash_profile
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;export PYTHONPATH=&amp;quot;/home/pi/software/Adafruit-Raspberry-Pi-Python-Code/Adafruit_PWM_Servo_Driver:$PYTHONPATH&amp;quot;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;where &lt;code&gt;software&lt;/code&gt; is the directory where I downloaded the software&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Since these programs are run with the super user, and running with super user erases the PYTHONPATH by default, we need to explicitly keep the PYTHONPATH when running as super user
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo visudo -f /etc/sudoers.d/custom&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;this creates a custom file rather than directly editing sudoers&lt;/li&gt;
&lt;li&gt;by default this uses nano to edit the file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;add &lt;code&gt;Defaults env_keep += &amp;quot;PYTHONPATH&amp;quot;&lt;/code&gt; to the file and exit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;running-the-tutorial-demo&#34;&gt;Running the tutorial demo&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Download the additional 
&lt;a href=&#34;https://github.com/wulfebw/robotics_rl/blob/master/tutorials/2_single_servo.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; used to control the servo
&lt;ul&gt;
&lt;li&gt;this is in the same github repo as the code from the previous tutorial&lt;/li&gt;
&lt;li&gt;it contains a utility function that allows for controlling the servo based on the desired angle of the servo arm rather than with pulse widths&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Run the tutorial demo
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo python 2_single_servo.py&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/kY3lgvHtju8&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Intermediate-Horizon Automotive Risk Prediction</title>
      <link>https://wulfebw.github.io/publication/risk-prediction/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/publication/risk-prediction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LED Control</title>
      <link>https://wulfebw.github.io/post/01-led-control/</link>
      <pubDate>Sun, 04 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/01-led-control/</guid>
      <description>&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We&amp;rsquo;ll make a LED blink in this post. Gotta start somewhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preview&#34;&gt;Preview&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/9AAlkAnyxGc?ecver=1&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h2 id=&#34;materials&#34;&gt;Materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Raspberry Pi
&lt;ul&gt;
&lt;li&gt;For example through CanaKit&lt;/li&gt;
&lt;li&gt;$70 on 
&lt;a href=&#34;https://www.amazon.com/CanaKit-Raspberry-Complete-Starter-Kit/dp/B01C6Q2GSY/ref=sr_1_1?s=electronics&amp;amp;ie=UTF8&amp;amp;qid=1517689480&amp;amp;sr=1-1&amp;amp;keywords=CanaKit&amp;#43;Raspberry&amp;#43;Pi&amp;#43;3&amp;#43;Complete&amp;#43;Starter&amp;#43;Kit&amp;#43;-&amp;#43;32&amp;#43;GB&amp;#43;Edition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;electric cables
&lt;ul&gt;
&lt;li&gt;$7 on 
&lt;a href=&#34;https://www.amazon.com/gp/product/B01LZF1ZSZ/ref=oh_aui_detailpage_o08_s00?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There are probably cheaper options&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;electronics start kit
&lt;ul&gt;
&lt;li&gt;$13 on 
&lt;a href=&#34;https://www.amazon.com/gp/product/B01ERP6WL4/ref=oh_aui_detailpage_o08_s00?ie=UTF8&amp;amp;psc=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;amazon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;There are definitely cheaper options&lt;/li&gt;
&lt;li&gt;For this tutorial, we&amp;rsquo;re just using an LED, a 1k resistor, and a breadboard&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;commonly available materials
&lt;ul&gt;
&lt;li&gt;keyboard&lt;/li&gt;
&lt;li&gt;mouse&lt;/li&gt;
&lt;li&gt;monitor&lt;/li&gt;
&lt;li&gt;hdmi-to-hdmi cable to connect raspberry pi to monitor&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;total cost: $90 + cost of common materials&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;walkthrough&#34;&gt;Walkthrough&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;set up the raspberry pi
&lt;ul&gt;
&lt;li&gt;see canakit 
&lt;a href=&#34;https://www.canakit.com/quick-start/pi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instructions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;set up raspberry pi wifi connection and ssh
&lt;ul&gt;
&lt;li&gt;connect pi to monitor, mouse, keyboard&lt;/li&gt;
&lt;li&gt;follow these 
&lt;a href=&#34;https://www.raspberrypi.org/documentation/remote-access/ssh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instructions&lt;/a&gt; to set up ssh
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;hostname -I&lt;/code&gt; to get ip address&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ssh pi@&amp;lt;ip address&amp;gt;&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;from your other computer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;default password is &lt;code&gt;raspberry&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;set up raspberry pi with LED
&lt;ul&gt;
&lt;li&gt;see Raspberry Pi Cookbook instructions 
&lt;a href=&#34;http://razzpisampler.oreilly.com/ch03.html#SEC7.1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;download my version of the 
&lt;a href=&#34;https://github.com/wulfebw/robotics_rl/blob/master/tutorials/1_led.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;run &lt;code&gt;sudo python 1_led.py&lt;/code&gt; from cmd line&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/9AAlkAnyxGc?ecver=1&#34; frameborder=&#34;0&#34; allow=&#34;autoplay; encrypted-media&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Robotics Intro</title>
      <link>https://wulfebw.github.io/post/00-robotics-intro/</link>
      <pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/00-robotics-intro/</guid>
      <description>&lt;h2 id=&#34;outline&#34;&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;This project consists of building a robotic arm with visual perception, and learning to control it through reinforcement and imitation learning&lt;/li&gt;
&lt;li&gt;Here&amp;rsquo;s the breakdown of the task:&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://wulfebw.github.io/post/01-led-control&#34;&gt;LED control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://wulfebw.github.io/post/02-single-servo-control&#34;&gt;Single-servo control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://wulfebw.github.io/post/03-two-link-programmatic-control&#34;&gt;Two-link programmatic control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Two-link manual control&lt;/li&gt;
&lt;li&gt;Two-link imitation control&lt;/li&gt;
&lt;li&gt;Perception&lt;/li&gt;
&lt;li&gt;Two-link RL control&lt;/li&gt;
&lt;li&gt;Building the robotic arm&lt;/li&gt;
&lt;li&gt;Robotic arm programmatic control&lt;/li&gt;
&lt;li&gt;Robotic arm manual control&lt;/li&gt;
&lt;li&gt;Robotic arm imitation control&lt;/li&gt;
&lt;li&gt;Robotic arm RL control&lt;/li&gt;
&lt;li&gt;Robotic arm sim2real RL control&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-is-this&#34;&gt;What is this?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;this is an outline for a project&lt;/li&gt;
&lt;li&gt;the project is to build a robotic arm and control it via reinforcement and imitation learning&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-am-i-doing-this-project&#34;&gt;Why am I doing this project?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;RL and stochastic dynamic programming have been successful in performing a number of tasks like game playing and robotics&lt;/li&gt;
&lt;li&gt;Most RL (human) learning resources focus on implementing algorithms and applying them in simulation&lt;/li&gt;
&lt;li&gt;Real-world robotics is a key application area of reinforcement learning because it is a case where RL is very likely the best option for learning control policies&lt;/li&gt;
&lt;li&gt;The goal of this project is to demonstrate the application of RL in a robotics task
&lt;ul&gt;
&lt;li&gt;In a manner that people could do by themselves at reasonable cost without external resources like a robotics lab&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Reinforcement Learning for Collision Avoidance</title>
      <link>https://wulfebw.github.io/project/uav-drl/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/project/uav-drl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Playing Atari with Hierarchical Deep Reinforcement Learning</title>
      <link>https://wulfebw.github.io/project/atari-hierarchical-drl/</link>
      <pubDate>Fri, 19 May 2017 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/project/atari-hierarchical-drl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DQN Tips</title>
      <link>https://wulfebw.github.io/post/dqn/</link>
      <pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/dqn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;update: see 
&lt;a href=&#34;https://arxiv.org/abs/1710.02298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rainbow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sanity check the implementation
&lt;ul&gt;
&lt;li&gt;come up with a simple dataset and see if the DQN can correctly learn values for it&lt;/li&gt;
&lt;li&gt;an example is a contextual bandit problem where you have two possible states, and two actions, where one action is +1 and the other -1&lt;/li&gt;
&lt;li&gt;generally, an rl method should work on 1-step and 2-step mdps both with and without random rewards and transitions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;updating the target network
&lt;ul&gt;
&lt;li&gt;check freeze rate&lt;/li&gt;
&lt;li&gt;this should be between every 100 updates and every 40000&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;batch size
&lt;ul&gt;
&lt;li&gt;this can be hard to set and depends on CPU vs GPU as well as the state dimensionality&lt;/li&gt;
&lt;li&gt;start with 32 and increase&lt;/li&gt;
&lt;li&gt;the goal should be to have each experience replayed some number of times on average
&lt;ul&gt;
&lt;li&gt;this is determined by the size of the replay memory and batch size&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;replay memory
&lt;ul&gt;
&lt;li&gt;replay memory size 1,000 to 10,000,000&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;action selection
&lt;ul&gt;
&lt;li&gt;softmax vs e-greedy
&lt;ul&gt;
&lt;li&gt;softmax typically works better if the temperature is tuned well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;learning rate
&lt;ul&gt;
&lt;li&gt;try values between 0.01 and 0.00001 generally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;state sampling
&lt;ul&gt;
&lt;li&gt;only relevant when initial-state distribution can be controlled&lt;/li&gt;
&lt;li&gt;if the important rewards are common then uniform sampling of the initial state might work&lt;/li&gt;
&lt;li&gt;if the important reward is rare then you&amp;rsquo;ll need to oversample these states
&lt;ul&gt;
&lt;li&gt;to do this in a principled manner, you need to know the relative probability of sampling states in the original MDP versus the proposal MDP (i.e., you need to do importance sampling)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;prioritized sampling can also help if you are dealing with rare, significant rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;normalizing input
&lt;ul&gt;
&lt;li&gt;mean center and normalize&lt;/li&gt;
&lt;li&gt;to get the stats, if it&amp;rsquo;s stationary, run it for 100,000 samples and compute mean and std dev&lt;/li&gt;
&lt;li&gt;or if the range of the input is available then you can use that
&lt;ul&gt;
&lt;li&gt;subtract the average of the end points&lt;/li&gt;
&lt;li&gt;divide by half the total range&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;max episode length
&lt;ul&gt;
&lt;li&gt;when ending an episode &lt;em&gt;not&lt;/em&gt; at a terminal state, be careful not to label that as a terminal state&lt;/li&gt;
&lt;li&gt;this value will impact the distribution over the state space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;terminal states
&lt;ul&gt;
&lt;li&gt;you need to handle this so that the target value is zero if it&amp;rsquo;s terminal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;discount
&lt;ul&gt;
&lt;li&gt;&amp;lt; 1 unless finite horizon in which case &amp;lt;= 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;initializing the network weights
&lt;ul&gt;
&lt;li&gt;this is consistently surprisingly important&lt;/li&gt;
&lt;li&gt;see &lt;a href=&#34;http://cs231n.github.io/neural-networks-2/&#34;&gt;http://cs231n.github.io/neural-networks-2/&lt;/a&gt; (Weight Initialization)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;action space
&lt;ul&gt;
&lt;li&gt;if this is large then it can make learning slow&lt;/li&gt;
&lt;li&gt;if this really should be continuous, then consider policy gradient methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regularization / dropout
&lt;ul&gt;
&lt;li&gt;many papers don&amp;rsquo;t report using regularization or dropout&lt;/li&gt;
&lt;li&gt;empirically it can help in certain situations
&lt;ul&gt;
&lt;li&gt;for example when the training MDP is different from the testing MDP and you need the policy to generalize&lt;/li&gt;
&lt;li&gt;start with small values for l2 reg (0.00001)&lt;/li&gt;
&lt;li&gt;dropout can vary dramatically in effectiveness (dropout ratio of 0.01 to 0.5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;network hyperparams
&lt;ul&gt;
&lt;li&gt;number of layers
&lt;ul&gt;
&lt;li&gt;start small (2) and increase as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;number of units
&lt;ul&gt;
&lt;li&gt;start small (32) and increase as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nonlinearity
&lt;ul&gt;
&lt;li&gt;start with relu or tanh&lt;/li&gt;
&lt;li&gt;maxout, elu, etc for getting extra benefit&lt;/li&gt;
&lt;li&gt;common pitfall: applying a nonlinearity to the last output of the network
&lt;ul&gt;
&lt;li&gt;this should just be a affine layer (dot(x,W) + b)&lt;/li&gt;
&lt;li&gt;in the relu case, applying it to the last layer makes all the output positive, which will break it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;optimizer
&lt;ul&gt;
&lt;li&gt;start with adam or rmsprop&lt;/li&gt;
&lt;li&gt;then check out (&lt;a href=&#34;http://cs231n.github.io/neural-networks-3/&#34;&gt;http://cs231n.github.io/neural-networks-3/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;rewards
&lt;ul&gt;
&lt;li&gt;you can clip them between -1 and +1, but you lose a lot of information&lt;/li&gt;
&lt;li&gt;if you have to use certain reward values larger than that, it can help to normalize them to be between -1 and +1&lt;/li&gt;
&lt;li&gt;if you have control over the rewards, then consider using reward shaping&lt;/li&gt;
&lt;li&gt;if the rewards are too large, then you can end up with e.g., relus dying&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dqn variations
&lt;ul&gt;
&lt;li&gt;double dqn works a lot better, particularly if the rewards are negative
&lt;ul&gt;
&lt;li&gt;this is particularly easy to implement, so it&amp;rsquo;s generally worth a shot&lt;/li&gt;
&lt;li&gt;(&lt;a href=&#34;https://arxiv.org/abs/1509.06461&#34;&gt;https://arxiv.org/abs/1509.06461&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dueling network can be better
&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;https://arxiv.org/abs/1509.06461&#34;&gt;https://arxiv.org/abs/1509.06461&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;replay memory variations
&lt;ul&gt;
&lt;li&gt;prioritized replay
&lt;ul&gt;
&lt;li&gt;oversamples certain experiences based on td-error and uses importance sampling in the loss to maintain unbiased state distribution&lt;/li&gt;
&lt;li&gt;helps a lot with sparse rewards&lt;/li&gt;
&lt;li&gt;(&lt;a href=&#34;https://arxiv.org/abs/1511.05952&#34;&gt;https://arxiv.org/abs/1511.05952&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Collision Avoidance for Unmanned Aircraft Using Coordination Tables</title>
      <link>https://wulfebw.github.io/publication/uav-coordination/</link>
      <pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/publication/uav-coordination/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Comparison of Deep and Traditional Reinforcement Learning Methods for Playing Atari</title>
      <link>https://wulfebw.github.io/project/atari-rl/</link>
      <pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/project/atari-rl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Language Modeling with Recurrent Generative Adversarial Networks</title>
      <link>https://wulfebw.github.io/project/language-modeling-gans/</link>
      <pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/project/language-modeling-gans/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Geo-localization of Street View Images</title>
      <link>https://wulfebw.github.io/project/geolocalization/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/project/geolocalization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Predicting Social Interaction Outcomes</title>
      <link>https://wulfebw.github.io/project/social/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/project/social/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
