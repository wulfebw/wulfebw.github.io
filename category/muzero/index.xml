<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>muzero | Blake Wulfe</title>
    <link>https://wulfebw.github.io/category/muzero/</link>
      <atom:link href="https://wulfebw.github.io/category/muzero/index.xml" rel="self" type="application/rss+xml" />
    <description>muzero</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 09 Dec 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wulfebw.github.io/img/index.png</url>
      <title>muzero</title>
      <link>https://wulfebw.github.io/category/muzero/</link>
    </image>
    
    <item>
      <title>MuZero</title>
      <link>https://wulfebw.github.io/post/muzero/</link>
      <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/muzero/</guid>
      <description>&lt;h1 id=&#34;context&#34;&gt;Context&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1911.08265.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MuZero paper.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/wulfebw/muzero&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tabular implementation.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;See that link for code description and some experiments.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;question&#34;&gt;Question&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Why should MuZero be better than other deep RL algorithms (in particular DQN)?&lt;/li&gt;
&lt;li&gt;The paper, 
&lt;a href=&#34;https://arxiv.org/pdf/1906.05243.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;When to use parametric models in reinforcement learning?&amp;quot;&lt;/a&gt;, goes into this question but with an emphasis on a 
&lt;a href=&#34;https://arxiv.org/pdf/1903.00374.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;different&lt;/a&gt; model-based algorithm
&lt;ul&gt;
&lt;li&gt;They define planning in an unusual manner as using &amp;ldquo;more computation without additional data to improve predictions and behavior&amp;rdquo;
&lt;ul&gt;
&lt;li&gt;The &amp;ldquo;usual&amp;rdquo; definition being the use of a (learned) transition and reward model to compute a value function or policy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The reasons they give for why parameteric model-based planning might outperform non-parameteric (experience replay) planning are not that clear to me, but may include:
&lt;ol&gt;
&lt;li&gt;Inductive bias of the parameteric model (improving generalization)&lt;/li&gt;
&lt;li&gt;Improved exploration through use of the model&lt;/li&gt;
&lt;li&gt;Improved credit assignment (&amp;ldquo;backward planning&amp;rdquo;)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To answer the question, I think the reason MuZero might outperfom DQN is the second reason: that it exhibits better exploration
&lt;ul&gt;
&lt;li&gt;This isn&amp;rsquo;t the typical kind of improved exploration through model use that you get with e.g., 
&lt;a href=&#34;https://ie.technion.ac.il/~moshet/brafman02a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;R-max&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Instead, because the learned value function is approximate, if the learned model is &amp;ldquo;good enough&amp;rdquo;, then you can use additional computation to select better actions in the environment than you would if you behaved greedily with respect to the value function itself.
&lt;ul&gt;
&lt;li&gt;Where &amp;ldquo;additional computation&amp;rdquo; means running MCTS.&lt;/li&gt;
&lt;li&gt;This means that the actions you take will, on average, be better.&lt;/li&gt;
&lt;li&gt;Which intuitively should mean that you&amp;rsquo;ll learn about the optimal policy and its value function more quickly.
&lt;ul&gt;
&lt;li&gt;And reach different parts of the state space that have higher value.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Which can be viewed as improved exploration.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;This works for the definition of &amp;ldquo;better&amp;rdquo; as &amp;ldquo;more sample efficient&amp;rdquo;
&lt;ul&gt;
&lt;li&gt;But that additional computation takes more resources and time, so if you defined &amp;ldquo;better&amp;rdquo; as the algorithm that provides the best solution given some amount of time, DQN might outperform MuZero.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
