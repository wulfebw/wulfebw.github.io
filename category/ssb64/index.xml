<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ssb64 | Blake Wulfe</title>
    <link>https://wulfebw.github.io/category/ssb64/</link>
      <atom:link href="https://wulfebw.github.io/category/ssb64/index.xml" rel="self" type="application/rss+xml" />
    <description>ssb64</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 05 Jun 2020 22:14:38 -0700</lastBuildDate>
    <image>
      <url>https://wulfebw.github.io/images/icon_hu74c5e83fe8339e265cecdcef41753095_57754_512x512_fill_lanczos_center_2.png</url>
      <title>ssb64</title>
      <link>https://wulfebw.github.io/category/ssb64/</link>
    </image>
    
    <item>
      <title>Super Smash Bros. 64 AI: Reinforcement Learning Part 1: The Environment</title>
      <link>https://wulfebw.github.io/post/ssb64-rl-01/</link>
      <pubDate>Fri, 05 Jun 2020 22:14:38 -0700</pubDate>
      <guid>https://wulfebw.github.io/post/ssb64-rl-01/</guid>
      <description>&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;The existing Nintendo 64 Reinforcement learning environments have some disadvantages. 
&lt;a href=&#34;https://github.com/openai/retro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gym-Retro&lt;/a&gt; can almost support N64 emulation, but not quite. I made some changes to the Gym-Retro code to allow for N64 environments. This post describes some of the challenges in doing that, initial results in training an agent in Super Smash Bros. 64, and next steps.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can find the code 
&lt;a href=&#34;https://github.com/wulfebw/retro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Here&amp;rsquo;s an example of an RL agent (Pikachu) playing the game AI (level 9 DK):&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/xlh6xhtkBRY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;environment&#34;&gt;Environment&lt;/h2&gt;
&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;
&lt;p&gt;There are some existing N64 RL environments. 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gym-Mupen64Plus&lt;/a&gt; is the main one. It works by running an N64 emulator, taking screenshots of the game, and parsing the screen to compute rewards. This has some issues. First, it means that the environment runs at a variable rate with respect to the agent (i.e., the time between taking an action and having it enacted in the environment is variable). Second, parsing the screen is brittle and not scalable (to more games) compared with reading values from RAM. Third, the screen-shotting mechanism has some disadvantages, for example it&amp;rsquo;s difficult to run multiple environments at once.&lt;/p&gt;
&lt;p&gt;The goal of this project was to implement an RL environment for N64 that didn&amp;rsquo;t have these issues, and that operated like a typical gym environment.&lt;/p&gt;
&lt;h3 id=&#34;challenges&#34;&gt;Challenges&lt;/h3&gt;
&lt;p&gt;When I started, I considered a few options for implementing the environment, and settled on adapting the 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gym-Retro&lt;/a&gt; codebase to allow for N64 emulation. I wasn&amp;rsquo;t sure when I started why Gym-Retro didn&amp;rsquo;t already support N64 games. It turns out the main reasons for that are (i) N64 uses dynamic memory locations for game data, and Gym-Retro is implemented assuming static memory locations and (ii) emulating N64 visuals requires adding OpenGL support (and even with this support emulation is fairly slow).&lt;/p&gt;
&lt;h4 id=&#34;n64-emulation&#34;&gt;N64 Emulation&lt;/h4&gt;
&lt;p&gt;Gym-Retro incorporates the console emulators (called &amp;ldquo;cores&amp;rdquo;, borrowing 
&lt;a href=&#34;https://www.libretro.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;libretro&lt;/a&gt; terminology) as subrepos using 
&lt;a href=&#34;https://github.com/ingydotnet/git-subrepo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;git-subrepo&lt;/a&gt;. I took this same approach, and added 
&lt;a href=&#34;https://github.com/libretro/mupen64plus-libretro-nx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mupen64Plus-Next&lt;/a&gt;. This seemed to be the most actively developed libretro N64 core.&lt;/p&gt;
&lt;h4 id=&#34;n64-dynamic-memory&#34;&gt;N64 dynamic memory&lt;/h4&gt;
&lt;p&gt;N64 games (at least Super Smash Bros. 64) store data in memory in dynamic locations, but the address of that data is stored in a constant location. For example, the health of player one might be stored (approximately) anywhere in the block of RAM from &lt;code&gt;0x80000000&lt;/code&gt; to &lt;code&gt;0x88000000&lt;/code&gt; (see 
&lt;a href=&#34;https://en.wikibooks.org/wiki/N64_Programming/Memory_mapping&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;details&lt;/a&gt; on N64 memory map); however, the pointer to this health value is always stored at &lt;code&gt;0x80000000&lt;/code&gt; + &lt;code&gt;0x000A50E8&lt;/code&gt; + &lt;code&gt;0x00000020&lt;/code&gt; + &lt;code&gt;0x0000004C&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Gym-Retro isn&amp;rsquo;t designed for this sort of dynamic memory lookup, which is (I believe) partially why N64 wasn&amp;rsquo;t supported. In order to handle this, I (approximately) copied the approach taken by 
&lt;a href=&#34;http://tasvideos.org/BizHawk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BizHawk&lt;/a&gt; of having a separate memory perform the data lookup, but implemented this in python instead of c++. One subtle aspect to this is that the data pointers refer to absolute locations in memory, but the memory available is just the RDRAM section of size &lt;code&gt;0x08000000&lt;/code&gt;. So reading a location in memory ends up looking like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Skipping the rest of the class...
def read_address(self, ptr):
    self.assert_valid_ram_address(ptr)
    addr_list = self.ram[ptr:ptr + self.addr_size]
    abs_addr = convert_byte_list_to_int(addr_list)
    rel_addr = abs_addr - self.rambase
    self.assert_valid_ram_address(rel_addr)
    return rel_addr
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I copied this approach from 
&lt;a href=&#34;https://github.com/Isotarge/ScriptHawk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ScriptHawk&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;opengl&#34;&gt;OpenGL&lt;/h4&gt;
&lt;p&gt;The second challenge in supporting N64 in Gym-Retro is that the N64 libretro core requires that the libretro frontend provide OpenGL support. This requires some effort to implement, but it also means that the environment will be fairly slow to simulate because rendering 3d graphics (even using OpenGL) is slower than rendering the 2d graphics used in the supported cores (like Atari). I know next to nothing about OpenGL, so I basically copied the minimal libretro frontend code from 
&lt;a href=&#34;https://github.com/heuripedes/sdlarch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;sdlarch&lt;/a&gt; into the Gym-Retro emulator class (though adapted it to use 
&lt;a href=&#34;https://www.glfw.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GLFW&lt;/a&gt; instead of 
&lt;a href=&#34;https://www.libsdl.org/release/SDL-1.2.15/docs/html/guidevideoopengl.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SDL&lt;/a&gt; because GLFW provided all the functionality I needed and is a lot smaller and could be directly incorporated into the project as a subrepo).&lt;/p&gt;
&lt;p&gt;Even though I just copied the sdlarch code into the emulator, it took some effort to figure out where everything should go. I couldn&amp;rsquo;t find much documentation on how to implement a libretro frontend with OpenGL support online.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The end result is an N64 RL environment using Gym-Retro. Running 16 environments in parallel (on 8 cores) I&amp;rsquo;m able to get 45 frames per second. This is much slower than, for example, 
&lt;a href=&#34;https://github.com/openai/procgen&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;procgen&lt;/a&gt;, which can apparently simulate &amp;ldquo;thousands of steps per second on a single core&amp;rdquo;. This might be part of the reason OpenAI moved away from retro to procgen (in addition to the generalization properties of procgen).&lt;/p&gt;
&lt;h2 id=&#34;initial-experiments&#34;&gt;Initial Experiments&lt;/h2&gt;
&lt;p&gt;After getting the environment running I ran some initial experiments to make sure it works. These are &amp;ldquo;initial&amp;rdquo; in that they involve training against the game AI instead of self-play.&lt;/p&gt;
&lt;h3 id=&#34;environment-setup&#34;&gt;Environment Setup&lt;/h3&gt;
&lt;p&gt;For these initial experiments, the environment is set up for a single match between Pikachu (the controlled agent) and a level 9 DK. DK acts deterministically (I wasn&amp;rsquo;t sure about this beforehand, but this is clear from the results), though items are included and occur randomly. Since DK&amp;rsquo;s behavior is determined by the items, DK also acts randomly, though in practice this doesn&amp;rsquo;t matter much.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Observation space is a stack of two color images. I started with grayscale images, but the agent didn&amp;rsquo;t perform well, I believe because Pikachu blended into the background too much. I think four frames would help since two frames seems not quite Markovian, but using four color frames slowed down training a fair amount. I also used color images because they might make it easier to identify items.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
  &lt;img src=&#34;grayscale.png&#34; alt=&#34;grayscale&#34;/&gt;
  &lt;figcaption&gt;Where&#39;s Pikachu?&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;Action space is a multi-discrete space: two dimensions for directional and one for the button press. I could have alternatively used a multi-class action space, but it would have come out to be size 36, which seemed fairly large.&lt;/li&gt;
&lt;li&gt;Reward Function is +1 for killing DK, -1 for dying, +damage dealt / 100. I initially started with -damage received / 100, but that resulted in too much avoidance behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I chose DK as the opponent because the goal is ultimately to play the trained AI, and I&amp;rsquo;d prefer to play with DK. It turns out this isn&amp;rsquo;t a great choice of opponent for a sanity check because DK can take a lot of damage without dying. This results in the agent needing a great deal of exploration to find the value in actually killing DK as opposed to just damaging him a bunch. This is also an issue because the item drops include health, which DK tends to pick up thereby extending the matches.&lt;/p&gt;
&lt;h3 id=&#34;agent&#34;&gt;Agent&lt;/h3&gt;
&lt;h4 id=&#34;reinforcement-learning-algorithm&#34;&gt;Reinforcement Learning Algorithm&lt;/h4&gt;
&lt;p&gt;I started out using the 
&lt;a href=&#34;https://github.com/openai/baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;baselines&lt;/a&gt; implementation of PPO. My plan was originally to compare at least with DQN, but it turns out that because DK acts deterministically (disregarding items) the policy PPO learns is basically a deterministic sequence. For this reason I suspect that 
&lt;a href=&#34;https://arxiv.org/abs/1709.06009&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brute&lt;/a&gt; would probably do better than PPO in this setting. This isn&amp;rsquo;t much of an issue because the real goal is to train an agent through self-play where a deterministic sequence would perform poorly. This uses the 
&lt;a href=&#34;https://arxiv.org/abs/1802.01561&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Impala CNN&lt;/a&gt; because the smaller CNNs didn&amp;rsquo;t work, which is consistent with some of the results on procgen.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;Training for 10 million steps took roughly 3 days on an 8-core machine with an RTX 2070. The plot of average episode reward during training shows fairly consistent improvement, though it trailed off towards the end:&lt;/p&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;img src=&#34;training.png&#34; alt=&#34;training curve&#34; width=&#34;600&#34;/&gt;
&lt;/figure&gt;
&lt;p&gt;Here are two separate matches. In both cases, Pikachu executes (what seems to be) an identical sequence of actions up until a certain point, after which it starts to improvise:&lt;/p&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/xlh6xhtkBRY&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;figcaption&gt;I think Pikachu might be trying to pick up an item here, but is just facing the wrong way.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure style=&#34;text-align:center;&#34;&gt;
    &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/YhzcUxDBPrE&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
    &lt;figcaption&gt;This one involved more improvisation. It seems like Pikachu has at least learned to perform an up-B maneuver when off the map.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;h3 id=&#34;environment-1&#34;&gt;Environment&lt;/h3&gt;
&lt;h4 id=&#34;observation-space&#34;&gt;Observation Space&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Vision: I think the visual case is approximately as performant as it&amp;rsquo;s going to get.&lt;/li&gt;
&lt;li&gt;RAM: the environment provides access to ram, but it currently requires writing a memory class for every game. This could be generalized so that the memory locations just need to be specified in a text file.&lt;/li&gt;
&lt;li&gt;Manual feature sets: 
&lt;a href=&#34;https://openai.com/projects/five/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI 5&lt;/a&gt; and 
&lt;a href=&#34;https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaStar&lt;/a&gt; operate on manually designed sets of features derived from RAM. It&amp;rsquo;s likely that if the goal is to build an RL agent for integration into SSB64 Mods that this is the approach that should be taken because training will be faster and deployment will be lighter-weight. Implementing this would require running the emulator without doing any screen rendering, which would likely require writing a mupen64plus visualization plugin that just does nothing. I suspect that this will be the only way to get good self-play agents working because rendering the screen is too slow and I don&amp;rsquo;t have that much computational resources for doing this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;action-space&#34;&gt;Action Space&lt;/h4&gt;
&lt;p&gt;A comparison of the multi-discrete action space with the multi-class action space might be worthwhile. Multi-class for example performed better in the 
&lt;a href=&#34;https://wulfebw.github.io/post/ssb64-bc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;behavioral cloning case&lt;/a&gt;, but that&amp;rsquo;s a somewhat large action space to use for reinforcement learning, which might be slower to learn than the multi-discrete option.&lt;/p&gt;
&lt;h4 id=&#34;games&#34;&gt;Games&lt;/h4&gt;
&lt;p&gt;The environment should work for all N64 games, though I believe each will require its own RAM specifications. Those are largely available via 
&lt;a href=&#34;https://github.com/Isotarge/ScriptHawk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ScriptHawk&lt;/a&gt;, though the values there differed from what I found in some cases.&lt;/p&gt;
&lt;h3 id=&#34;agent-1&#34;&gt;Agent&lt;/h3&gt;
&lt;p&gt;A Recurrent policy might be worth trying. There are also questions about how the policy will work in the self-play setting. For example, how will it know which agent it is controlling? Chances are it won&amp;rsquo;t be able to based on the screen alone and some additional information will have to be provided via the RAM.&lt;/p&gt;
&lt;p&gt;A comparison between DQN, MuZero, and other model-based methods could be interesting. DQN might be more sample efficient, and that would help because the interaction here is the bottleneck. MuZero could be interesting because this is effectively a competitive environment, and explicitly modeling it as one might improve performance. A model-based approach using 
&lt;a href=&#34;https://arxiv.org/abs/1911.12247&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contrasitve Learning of Structured World Models&lt;/a&gt; (C-SWM) could help improve sample efficiency via an object-based inductive bias. I tried this out on the behavioral cloning dataset and it seems to give some interesting results:&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;ssb64-cswm.gif&#34; alt=&#34;ssb64-cswm&#34; width=&#34;200&#34;/&gt;
    &lt;figcaption&gt;C-SWM applied to a SSB64 dataset.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It seems to separate out the foreground from the background, but not the agents. I think correcting for the camera movement could fix that. Learning the model for planning purposes like in MuZero might also put greater emphasis on the agents.&lt;/p&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;The goal is to be able to play an agent trained through self-play. You&amp;rsquo;d ideally be able to use a controller. So I&amp;rsquo;ll need to write a player agent that takes input from a controller and passes that to the gym environment. This shouldn&amp;rsquo;t be too hard using the inputs 
&lt;a href=&#34;https://github.com/zeth/inputs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;package&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;lessons&#34;&gt;Lessons&lt;/h2&gt;
&lt;h3 id=&#34;gym-retro&#34;&gt;Gym-Retro&lt;/h3&gt;
&lt;p&gt;Here&amp;rsquo;s how this works: the python interface is provided via 
&lt;a href=&#34;https://github.com/pybind/pybind11&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pybind11&lt;/a&gt; (see retro.cpp). retro.cpp has an Emulator object defined in emulator.h/cpp. The emulator class is actually a libretro frontend, though it only implements some of the frontend functionality. This class interacts with libretro cores by dynamically loading functions from them. The integration UI is a UI on top of the emulator with some search functionality. The search functionality doesn&amp;rsquo;t work with N64 due to its use of dynamic memory locations.&lt;/p&gt;
&lt;h3 id=&#34;bugs--things-that-took-a-long-time-to-figure-out&#34;&gt;Bugs / things that took a long time to figure out&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;One bug I spent a long time on before I realized that N64 used dynamic memory locations was getting the search functionality in the integration UI to work. There was a bug where the rambase was loaded from a file as a signed int, which when converted to a size_t gave an incorrect value, but only if the rambase was large enough. Probably spent 2 hours on that.&lt;/li&gt;
&lt;li&gt;I typically use pytorch, but was using tensorflow since baselines uses it. Instead of using baselines&#39; &lt;code&gt;run&lt;/code&gt; function I just implemented my own in a script, but didn&amp;rsquo;t look at &lt;code&gt;run&lt;/code&gt; carefully and missed the part where they set a specific tensorflow gpu config. 
&lt;a href=&#34;https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apparently&lt;/a&gt; tensorflow expands to use all (?) of the gpu memory. This resulted in mysterious segfaults from OpenGL that were difficult to debug.&lt;/li&gt;
&lt;li&gt;Figuring out the format of the &lt;code&gt;n64.json&lt;/code&gt; file took a long time. I don&amp;rsquo;t think there are docs on how this is defined. So here&amp;rsquo;s what everything is as far as I can tell:
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lib&lt;/code&gt;: this should be a prefix of the name of the shared library the core is compiled to. So for instance mupen64plus is compiled to mupen64plus_next_libretro.so. So &lt;code&gt;lib&lt;/code&gt; should be set to &amp;ldquo;mupen64plus_next&amp;rdquo;. Furthermore, after compiling the core each time, you need to copy the shared library to retro/retro/cores. This is done by setup.py initially, but if you recompile you have to do it manually.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ext&lt;/code&gt;: extension of the rom files for this core.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;types&lt;/code&gt;: I believe this is only used by the searching functionality to limit the scope of its search for variables to the specified types.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rambase&lt;/code&gt;: the location in memory where the relevant portion of ram starts. In the n64 this is 0x80000000. This has an effect on how the emulator reads the ram so you have to set it correctly. To find out what value to use, look up the &amp;ldquo;memory map&amp;rdquo; for your core.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;keybinds&lt;/code&gt;: these are the keyboard keys used to manually control the environment.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;buttons&lt;/code&gt;: these are the actual button values used to take actions, and (I believe) their order does matter. It needs to match the order used in the core/libretro.h file. So for example, atari2600/libretro.h:178 shows the button ordering. The buttons in this json file need to match that. All the libretro cores implement a similar ordering it seems.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;actions&lt;/code&gt;: each row is a dimension of the action space (in the multi-discrete case). Within each dimension, each element is an action in that dimension. Within each action, the elements are the buttons that result in that action.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;What to do about the analog stick in N64? The analog stick of the other cores just provides directionals, but in N64 it provides the magnitude of the directional, and that has an influence on the behavior in the game. I ended up changing the core to map these fine-grain controls to directionals.&lt;/li&gt;
&lt;li&gt;MAX_PLAYERS: The default MAX_PLAYERS value is 2, but for N64 it should be 4. This resulted in the other players executing random commands when running the game and it took me probably an hour to figure out why that was happening.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main tool I used to figure that stuff out was gdb, but debugging a c program called through python, which is not something I realized you could do previously.&lt;/p&gt;
&lt;h3 id=&#34;n64-emulation-visualization-plugins&#34;&gt;N64 Emulation Visualization Plugins&lt;/h3&gt;
&lt;p&gt;When I first got the emulator working and realized that emulator.h/cpp didn&amp;rsquo;t implement the OpenGL functionality I tried to avoid implementing it myself by using software-defined rendering in the emulator. The library for doing this is referred to as angrylion, and it actually works fine, except that it&amp;rsquo;s very slow, running at about 8 fps compared with OpenGL&amp;rsquo;s 45 fps. However, getting an end-to-end system running with angrylion, and then going back to tackle the relatively difficult task (for me) of implementing the OpenGL functionality for libretro frontends was (accidentally) a good strategy because I probably would have given up if I had had to implement the OpenGL stuff to get anything working. This is mainly because with the angrylion version already implemented, I knew what &amp;ldquo;working&amp;rdquo; looked like, and I also was much more familiar with the system by the time I tried to tackle the OpenGL stuff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Super Smash Bros. 64 AI: Behavioral Cloning</title>
      <link>https://wulfebw.github.io/post/ssb64-bc/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/ssb64-bc/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Goal&lt;/strong&gt;: develop a Super Smash Bros. 64 AI that acts based on image input&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Approach&lt;/strong&gt;: start with behavioral cloning since it&amp;rsquo;s simple&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: Images and action sequences, and training and validation splits available 
&lt;a href=&#34;https://s3.console.aws.amazon.com/s3/buckets/ssb64-data-public/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; (
&lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;s3 bucket&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model and Training&lt;/strong&gt;: Convolutional and recurrent neural networks, implemented in pytorch, code available 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deployment&lt;/strong&gt;: Through 
&lt;a href=&#34;https://github.com/mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gym-mupen64plus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Quantitative gameplay performance (damage, kills, lives lost)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demo&lt;/strong&gt;: See the description of the video for some interesting events&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Usr00SPbRHg&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;!-- markdown-toc start - Don&#39;t edit this section. Run M-x markdown-toc-refresh-toc --&gt;
&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#related-work&#34;&gt;Related work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#approach&#34;&gt;Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#experiments-and-results&#34;&gt;Experiments and Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-1-dataset-collection&#34;&gt;Appendix 1: Dataset collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-2-imitation-learning&#34;&gt;Appendix 2: Imitation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-3-model-and-training-details&#34;&gt;Appendix 3: Model and training details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4: N64 as an RL environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#appendix-5-rnns-why-use-them-how-to-optimize-and-regularize-them&#34;&gt;Appendix 5: RNNs why use them, how to optimize and regularize them&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- markdown-toc end --&gt;
&lt;h1 id=&#34;motivation&#34;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Build a Super Smash Bros. 64 (SSB64) AI agent to play against.&lt;/p&gt;
&lt;h1 id=&#34;related-work&#34;&gt;Related work&lt;/h1&gt;
&lt;h2 id=&#34;n64-and-ssb64-ai&#34;&gt;N64 and SSB64 AI&lt;/h2&gt;
&lt;p&gt;The only info on an SSB64 AI I could find online was this cs231n 
&lt;a href=&#34;http://cs231n.stanford.edu/reports/2016/pdfs/113_Report.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;project&lt;/a&gt; with associated 
&lt;a href=&#34;https://www.youtube.com/watch?v=c-6XcmM-MSk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;videos&lt;/a&gt;. It&amp;rsquo;s a nice write up and was helpful in doing this project.&lt;/p&gt;
&lt;p&gt;I thought implementing a similar project myself would be worth doing nevertheless because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;They don&amp;rsquo;t provide access to any data or code&lt;/li&gt;
&lt;li&gt;Their agent seems to behave a bit repetitively&lt;/li&gt;
&lt;li&gt;I was curious whether a different action-space or model would work better&lt;/li&gt;
&lt;li&gt;As stated, I wanted to play a better AI myself&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/kevinhughes27/TensorKart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorKart&lt;/a&gt; is a project that developed a Mario Kart 64 AI. That project (and associated blog 
&lt;a href=&#34;https://www.kevinhughes.ca/blog/tensor-kart&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post&lt;/a&gt;) was also helpful, and provided the technical approach roughly followed in this project.&lt;/p&gt;
&lt;h2 id=&#34;game-ai-in-general&#34;&gt;Game AI in general&lt;/h2&gt;
&lt;p&gt;There&amp;rsquo;s a ton of work on game playing. Most of it focuses on reinforcement learning (RL). This is because RL / approximate dynamic programming is currently the best approach to the problem. See for example 
&lt;a href=&#34;https://openai.com/five/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI five&lt;/a&gt; or Deepmind 
&lt;a href=&#34;https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;starcraft II AI &lt;/a&gt; or 
&lt;a href=&#34;https://deepmind.com/research/alphago/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaGo&lt;/a&gt;. RL is the best approach in these game playing scenarios because (among other reasons):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It&amp;rsquo;s possible to collect a large amount of training data through simulation.&lt;/li&gt;
&lt;li&gt;The training and testing distributions closely or exactly match.&lt;/li&gt;
&lt;li&gt;The agent can benefit from 
&lt;a href=&#34;https://openai.com/blog/competitive-self-play/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;competitive self play&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s an easily accessible reward function (e.g., win/lose, damage, deaths) that aligns well with the true goal (developing a competitive agent).&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;p&gt;The dataset consists of a set of matches. See 
&lt;a href=&#34;#appendix-1-dataset-collection&#34;&gt;Appendix 1&lt;/a&gt; for details. Main points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://mupen64plus.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mupen64plus&lt;/a&gt; was used for emulating the N64.&lt;/li&gt;
&lt;li&gt;The dataset consists of a set of gameplay matches.
&lt;ul&gt;
&lt;li&gt;In total about 175,000 frames representing about 6 hours of gameplay.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Matches were played between blue DK and the highest level game AI Kirby.
&lt;ul&gt;
&lt;li&gt;This allows the network to identify which agent it controls.&lt;/li&gt;
&lt;li&gt;Extending this to multiple characters would require e.g., adding a feature indicating the controlled character.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;All matches were played on the dreamland stage.&lt;/li&gt;
&lt;li&gt;Items were included to make it more interesting.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&amp;rsquo;s a short clip selected from the dataset to give a sense for the behavior being imitated:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/K4bJlsvCliw&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;h1 id=&#34;approach&#34;&gt;Approach&lt;/h1&gt;
&lt;p&gt;I started with behavioral cloning (BC), which is supervised learning using a dataset of expert demonstrations, despite it being a worse-performing approach than RL because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It&amp;rsquo;s simpler to implement than RL methods.
&lt;ul&gt;
&lt;li&gt;Allows for becoming familiar with the emulator in a simplified setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;It provides a nice baseline for comparison.&lt;/li&gt;
&lt;li&gt;It has more predictable computational costs than RL.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The main disadvantages of behavioral cloning are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It may take a lot of data to achieve good performance (see 
&lt;a href=&#34;#appendix-2-imitation-learning&#34;&gt;Appendix 2&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;The resulting agent will not perform significantly better than the expert providing the data.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;It might perform slightly better due to decreased reaction time, but that&amp;rsquo;s not particularly interesting or fun to play against.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;observation-space&#34;&gt;Observation space&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A requirement for this project was that the AI has to act on image observations.
&lt;ul&gt;
&lt;li&gt;There are some alternatives to this observation space (e.g., operating on the game 
&lt;a href=&#34;https://deepsense.ai/playing-atari-on-ram-with-deep-q-learning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAM&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A single image is not a Markovian representation of the state.
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s unclear to what extent multiple images would improve performance.&lt;/li&gt;
&lt;li&gt;For this reason, different numbers of frames were considered as input (fixed history in the CNN case, and complete history in theory in the RNN case).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;action-space&#34;&gt;Action space&lt;/h2&gt;
&lt;p&gt;I considered two action spaces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Exclusive multiclass action space consisting of common SSB64 moves.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This is what the cs231n project uses.&lt;/li&gt;
&lt;li&gt;The actions include  the buttons, the axes, and smash combinations (A + left, A + right, etc).&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;A multi-discrete action space with a value for each joystick axis, and one value for the button.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This action space seemed like it should allow for more efficient learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;models&#34;&gt;Models&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;A Resnet-9, multi-frame model.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Using a four-frame history of grayscale images.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;An RNN with Resnet feature extractor.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This takes as input individual grayscale images at each timestep.&lt;/li&gt;
&lt;li&gt;Same visual feature extractor as the fixed-length history model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The models were implemented in pytorch. See 
&lt;a href=&#34;#appendix-3-model-and-training-details&#34;&gt;Appendix 3&lt;/a&gt; for training details.&lt;/p&gt;
&lt;h1 id=&#34;experiments-and-results&#34;&gt;Experiments and Results&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Edit (05/18/2020): I never ran this evaluation because gym-mupen64plus had a number of issues with it
&lt;ul&gt;
&lt;li&gt;See the appendix 4 for more details&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deployment&#34;&gt;Deployment&lt;/h3&gt;
&lt;p&gt;The models were deployed using 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gym-mupen64plus&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;conclusion-and-future-work&#34;&gt;Conclusion and Future Work&lt;/h1&gt;
&lt;p&gt;Despite not being the best approach to the problem, behavioral cloning was a helpful place to start in creating a SSB64 AI. I learned a fair amount about emulating the game (see 
&lt;a href=&#34;#appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4&lt;/a&gt;), 
&lt;a href=&#34;https://github.com/pytorch/ignite&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pytorch ignite&lt;/a&gt; (see 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc/blob/master/scripts/train.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;training script&lt;/a&gt;), regularizing RNNs (see 
&lt;a href=&#34;#appendix-5-rnns-why-use-them-how-to-optimize-and-regularize-them&#34;&gt;Appendix 5&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;
The main direction for future work is applying RL to the problem. As discussed in 
&lt;a href=&#34;#appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4&lt;/a&gt;, the N64 isn&amp;rsquo;t the most convenient environment for RL for technical reasons, so most of the effort in applying RL would be in creating a convenient environment.&lt;/p&gt;
&lt;h1 id=&#34;appendix-1-dataset-collection&#34;&gt;Appendix 1: Dataset collection&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Images were collected by adapting the TensorKart codebase
&lt;ul&gt;
&lt;li&gt;See this script&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I collected the data on a mac. The 
&lt;a href=&#34;https://github.com/zeth/inputs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Inputs&lt;/a&gt; python package used in TensorKart doesn&amp;rsquo;t work on mac for joysticks, so I made some changes to the input plugin for mupen64plus to accept a command line argument for a filepath at which to store the actions received. See 
&lt;a href=&#34;https://github.com/wulfebw/mupen64plus-input-sdl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for the fork containing those changes.&lt;/li&gt;
&lt;li&gt;Image-action pairs were created by finding, for each image, the non-null action that occurred most recently following the collected image, up to a maximum of 0.05 seconds after the image, or if no non-null action occurred, a noop action was selected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-2-imitation-learning&#34;&gt;Appendix 2: Imitation Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Some &lt;a href=&#34;https://wulfebw.github.io/assets/imitation_learning_notes.pdf&#34;&gt;notes&lt;/a&gt; on the topic.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-3-model-and-training-details&#34;&gt;Appendix 3: Model and training details&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;See &lt;a href=&#34;https://github.com/wulfebw/ssb64bc/blob/master/scripts/hparams.py&#34;&gt;here&lt;/a&gt; for the hyperparams used in the CNN case.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-4-n64-as-an-rl-environment&#34;&gt;Appendix 4: N64 as an RL environment&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;mupen64plus seems to be the most popular N64 emulator.&lt;/li&gt;
&lt;li&gt;It is not designed for use in RL.
&lt;ul&gt;
&lt;li&gt;The design is based on a core module that has plugins for video, audio, controller input, etc.&lt;/li&gt;
&lt;li&gt;The plugins don&amp;rsquo;t communicate directly.&lt;/li&gt;
&lt;li&gt;Providing a unified interface for RL would require a significant amount of effort, with the implementation options being:
&lt;ol&gt;
&lt;li&gt;Wrap the core and all plugins in an adapter program.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;And also interpret / provide access to the RAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Use 
&lt;a href=&#34;http://tasvideos.org/BizHawk.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BizHawk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Bizhawk seems to have already 
&lt;a href=&#34;https://github.com/TASVideos/BizHawk/tree/master/libmupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;done&lt;/a&gt; option 1, and is typically used in tool-assisted SSB64 gameplay.&lt;/li&gt;
&lt;li&gt;It only seems to work on windows for now, with linux support being 
&lt;a href=&#34;https://github.com/TASVideos/BizHawk/issues/1430&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;developed&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Alternatively, you could use 
&lt;a href=&#34;https://github.com/bzier/gym-mupen64plus&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gym-mupen64plus&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;It takes screenshots of the game and parses them to compute rewards.&lt;/li&gt;
&lt;li&gt;The main problems are:
&lt;ol&gt;
&lt;li&gt;Because the observations are from screenshots, the hardware might influence the rate of observations and their alignment with actions taken.
&lt;ul&gt;
&lt;li&gt;This is complicated to deal with, as can be seen in the dataset 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc/tree/master/ssb64bc/formatting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;formatting details&lt;/a&gt; of this project.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;gym-mupen64plus for SSB64 attempts to automatically select characters in versus mode, but that didn&amp;rsquo;t work in my case.
&lt;ul&gt;
&lt;li&gt;The implementation assumes the selector for the AI appears in the same location every time.
&lt;ul&gt;
&lt;li&gt;In my emulation of the game it was random.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Directly performing these tasks by operating on the RAM is clearly a better approach.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Parsing the screen for the reward seems to work, but breaks with small changes.
&lt;ul&gt;
&lt;li&gt;Using the RAM directly would be faster and more reliable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;appendix-5-rnns-why-use-them-how-to-optimize-and-regularize-them&#34;&gt;Appendix 5: RNNs why use them, how to optimize and regularize them&lt;/h1&gt;
&lt;h2 id=&#34;why-use-an-rnn&#34;&gt;Why use an RNN?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I tried using an RNN because I was curious to what extent it would improve performance.
&lt;ul&gt;
&lt;li&gt;The reasons it might improve performance are:
&lt;ol&gt;
&lt;li&gt;A reasonable-size, fixed-frame history may not represent a Markovian representation of the environment.&lt;/li&gt;
&lt;li&gt;The inductive bias of the RNN may provide some benefits (e.g., consistency or smoothness).&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;optimizing-and-regularizing-rnns&#34;&gt;Optimizing and regularizing RNNs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;First, note that RNNs have gone out of fashion lately (at least in NLP).
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1706.03762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformer networks&lt;/a&gt; are generally being preferred.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=Hygxb2CqKm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&amp;rsquo;s&lt;/a&gt; a paper that tries to explain why.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There are a lot of tricks for regularizing and optimizing RNNs.
&lt;ul&gt;
&lt;li&gt;This is a good 
&lt;a href=&#34;https://arxiv.org/pdf/1708.02182.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; on it that is somewhat recent.
&lt;ul&gt;
&lt;li&gt;It focuses on NLP, but the advice seemed to help in my case.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;I implemented or copied some of the regularization methods 
&lt;a href=&#34;https://github.com/wulfebw/ssb64bc/blob/master/ssb64bc/models/torch_utils.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;One decision to make in RNN training is whether to use &amp;ldquo;stateful&amp;rdquo; or &amp;ldquo;stateless&amp;rdquo; training.
&lt;ul&gt;
&lt;li&gt;See this 
&lt;a href=&#34;https://keras.io/examples/lstm_stateful/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;description&lt;/a&gt; and this 
&lt;a href=&#34;http://philipperemy.github.io/keras-stateful-lstm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explanation&lt;/a&gt;.
&lt;ul&gt;
&lt;li&gt;The gist is that you need to use stateful (maintaining the hidden state across batches) training if you want the network to learn to maintain memory beyond the length of the BPTT.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
