<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dqn | Blake Wulfe</title>
    <link>https://wulfebw.github.io/category/dqn/</link>
      <atom:link href="https://wulfebw.github.io/category/dqn/index.xml" rel="self" type="application/rss+xml" />
    <description>dqn</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 15 Apr 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://wulfebw.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>dqn</title>
      <link>https://wulfebw.github.io/category/dqn/</link>
    </image>
    
    <item>
      <title>DQN Tips</title>
      <link>https://wulfebw.github.io/post/dqn/</link>
      <pubDate>Sat, 15 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://wulfebw.github.io/post/dqn/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;update: see 
&lt;a href=&#34;https://arxiv.org/abs/1710.02298&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rainbow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;sanity check the implementation
&lt;ul&gt;
&lt;li&gt;come up with a simple dataset and see if the DQN can correctly learn values for it&lt;/li&gt;
&lt;li&gt;an example is a contextual bandit problem where you have two possible states, and two actions, where one action is +1 and the other -1&lt;/li&gt;
&lt;li&gt;generally, an rl method should work on 1-step and 2-step mdps both with and without random rewards and transitions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;updating the target network
&lt;ul&gt;
&lt;li&gt;check freeze rate&lt;/li&gt;
&lt;li&gt;this should be between every 100 updates and every 40000&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;batch size
&lt;ul&gt;
&lt;li&gt;this can be hard to set and depends on CPU vs GPU as well as the state dimensionality&lt;/li&gt;
&lt;li&gt;start with 32 and increase&lt;/li&gt;
&lt;li&gt;the goal should be to have each experience replayed some number of times on average
&lt;ul&gt;
&lt;li&gt;this is determined by the size of the replay memory and batch size&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;replay memory
&lt;ul&gt;
&lt;li&gt;replay memory size 1,000 to 10,000,000&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;action selection
&lt;ul&gt;
&lt;li&gt;softmax vs e-greedy
&lt;ul&gt;
&lt;li&gt;softmax typically works better if the temperature is tuned well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;learning rate
&lt;ul&gt;
&lt;li&gt;try values between 0.01 and 0.00001 generally&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;state sampling
&lt;ul&gt;
&lt;li&gt;only relevant when initial-state distribution can be controlled&lt;/li&gt;
&lt;li&gt;if the important rewards are common then uniform sampling of the initial state might work&lt;/li&gt;
&lt;li&gt;if the important reward is rare then you&amp;rsquo;ll need to oversample these states
&lt;ul&gt;
&lt;li&gt;to do this in a principled manner, you need to know the relative probability of sampling states in the original MDP versus the proposal MDP (i.e., you need to do importance sampling)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;prioritized sampling can also help if you are dealing with rare, significant rewards&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;normalizing input
&lt;ul&gt;
&lt;li&gt;mean center and normalize&lt;/li&gt;
&lt;li&gt;to get the stats, if it&amp;rsquo;s stationary, run it for 100,000 samples and compute mean and std dev&lt;/li&gt;
&lt;li&gt;or if the range of the input is available then you can use that
&lt;ul&gt;
&lt;li&gt;subtract the average of the end points&lt;/li&gt;
&lt;li&gt;divide by half the total range&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;max episode length
&lt;ul&gt;
&lt;li&gt;when ending an episode &lt;em&gt;not&lt;/em&gt; at a terminal state, be careful not to label that as a terminal state&lt;/li&gt;
&lt;li&gt;this value will impact the distribution over the state space&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;terminal states
&lt;ul&gt;
&lt;li&gt;you need to handle this so that the target value is zero if it&amp;rsquo;s terminal&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;discount
&lt;ul&gt;
&lt;li&gt;&amp;lt; 1 unless finite horizon in which case &amp;lt;= 1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;initializing the network weights
&lt;ul&gt;
&lt;li&gt;this is consistently surprisingly important&lt;/li&gt;
&lt;li&gt;see &lt;a href=&#34;http://cs231n.github.io/neural-networks-2/&#34;&gt;http://cs231n.github.io/neural-networks-2/&lt;/a&gt; (Weight Initialization)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;action space
&lt;ul&gt;
&lt;li&gt;if this is large then it can make learning slow&lt;/li&gt;
&lt;li&gt;if this really should be continuous, then consider policy gradient methods&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regularization / dropout
&lt;ul&gt;
&lt;li&gt;many papers don&amp;rsquo;t report using regularization or dropout&lt;/li&gt;
&lt;li&gt;empirically it can help in certain situations
&lt;ul&gt;
&lt;li&gt;for example when the training MDP is different from the testing MDP and you need the policy to generalize&lt;/li&gt;
&lt;li&gt;start with small values for l2 reg (0.00001)&lt;/li&gt;
&lt;li&gt;dropout can vary dramatically in effectiveness (dropout ratio of 0.01 to 0.5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;network hyperparams
&lt;ul&gt;
&lt;li&gt;number of layers
&lt;ul&gt;
&lt;li&gt;start small (2) and increase as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;number of units
&lt;ul&gt;
&lt;li&gt;start small (32) and increase as needed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nonlinearity
&lt;ul&gt;
&lt;li&gt;start with relu or tanh&lt;/li&gt;
&lt;li&gt;maxout, elu, etc for getting extra benefit&lt;/li&gt;
&lt;li&gt;common pitfall: applying a nonlinearity to the last output of the network
&lt;ul&gt;
&lt;li&gt;this should just be a affine layer (dot(x,W) + b)&lt;/li&gt;
&lt;li&gt;in the relu case, applying it to the last layer makes all the output positive, which will break it&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;optimizer
&lt;ul&gt;
&lt;li&gt;start with adam or rmsprop&lt;/li&gt;
&lt;li&gt;then check out (&lt;a href=&#34;http://cs231n.github.io/neural-networks-3/&#34;&gt;http://cs231n.github.io/neural-networks-3/&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;rewards
&lt;ul&gt;
&lt;li&gt;you can clip them between -1 and +1, but you lose a lot of information&lt;/li&gt;
&lt;li&gt;if you have to use certain reward values larger than that, it can help to normalize them to be between -1 and +1&lt;/li&gt;
&lt;li&gt;if you have control over the rewards, then consider using reward shaping&lt;/li&gt;
&lt;li&gt;if the rewards are too large, then you can end up with e.g., relus dying&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dqn variations
&lt;ul&gt;
&lt;li&gt;double dqn works a lot better, particularly if the rewards are negative
&lt;ul&gt;
&lt;li&gt;this is particularly easy to implement, so it&amp;rsquo;s generally worth a shot&lt;/li&gt;
&lt;li&gt;(&lt;a href=&#34;https://arxiv.org/abs/1509.06461&#34;&gt;https://arxiv.org/abs/1509.06461&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;dueling network can be better
&lt;ul&gt;
&lt;li&gt;(&lt;a href=&#34;https://arxiv.org/abs/1509.06461&#34;&gt;https://arxiv.org/abs/1509.06461&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;replay memory variations
&lt;ul&gt;
&lt;li&gt;prioritized replay
&lt;ul&gt;
&lt;li&gt;oversamples certain experiences based on td-error and uses importance sampling in the loss to maintain unbiased state distribution&lt;/li&gt;
&lt;li&gt;helps a lot with sparse rewards&lt;/li&gt;
&lt;li&gt;(&lt;a href=&#34;https://arxiv.org/abs/1511.05952&#34;&gt;https://arxiv.org/abs/1511.05952&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
